{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load bitsandbytes native library: /lib64/libc.so.6: version `GLIBC_2.34' not found (required by /mnt/lustre/work/oh/owl661/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/lustre/work/oh/owl661/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/mnt/lustre/work/oh/owl661/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "  File \"/mnt/lustre/work/oh/owl661/lib/python3.10/ctypes/__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/mnt/lustre/work/oh/owl661/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /lib64/libc.so.6: version `GLIBC_2.34' not found (required by /mnt/lustre/work/oh/owl661/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Please replace '<YOUR-TRAINING-PLACEHOLDER-TOKEN>' with the actual placeholder token used during training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from typing import Optional\n",
    "\n",
    "def load_pipeline_with_ti_from_local_folder_step(\n",
    "    local_embeddings_folder: str,\n",
    "    base_model_name_or_path: str,\n",
    "    placeholder_token: str,\n",
    "    training_step: int,\n",
    "    torch_dtype: torch.dtype = torch.float16,\n",
    "    device: Optional[str] = None,\n",
    ") -> StableDiffusion3Pipeline:\n",
    "    \"\"\"\n",
    "    Loads a Stable Diffusion 3 pipeline and injects Textual Inversion embeddings\n",
    "    from a local folder, for a specific training step.\n",
    "\n",
    "    Args:\n",
    "        local_embeddings_folder (str): Path to the local folder containing\n",
    "            step-specific embedding files like 'learned_embeds_t1-steps-XXXX.safetensors'.\n",
    "        base_model_name_or_path (str): The identifier of the base SD3 model.\n",
    "        placeholder_token (str): The placeholder token used during TI training.\n",
    "        training_step (int): The specific training step for which to load embeddings.\n",
    "        torch_dtype (torch.dtype): The torch dtype for loading the pipeline.\n",
    "        device (Optional[str]): The device to move the pipeline to (e.g., \"cuda\").\n",
    "            Defaults to \"cuda\" if available, else \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        StableDiffusion3Pipeline: The loaded pipeline with TI embeddings.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If any of the required embedding files are not found for the step.\n",
    "        ValueError: If placeholder_token or training_step is not provided or invalid.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if not placeholder_token:\n",
    "        raise ValueError(\"placeholder_token must be provided.\")\n",
    "    if not isinstance(training_step, int) or training_step <= 0:\n",
    "        raise ValueError(\"training_step must be a positive integer.\")\n",
    "    print(f\"Using placeholder token: '{placeholder_token}' for training step: {training_step}\")\n",
    "\n",
    "    # Define the filename templates\n",
    "    embedding_files_templates = [\n",
    "        {\"template\": \"learned_embeds_t1-steps-{}.safetensors\", \"encoder_attr\": \"text_encoder\", \"tokenizer_attr\": \"tokenizer\"},\n",
    "        {\"template\": \"learned_embeds_t2-steps-{}.safetensors\", \"encoder_attr\": \"text_encoder_2\", \"tokenizer_attr\": \"tokenizer_2\"},\n",
    "        {\"template\": \"learned_embeds_t3-steps-{}.safetensors\", \"encoder_attr\": \"text_encoder_3\", \"tokenizer_attr\": \"tokenizer_3\"},\n",
    "    ]\n",
    "\n",
    "    # Load the base pipeline\n",
    "    print(f\"Loading base model '{base_model_name_or_path}'...\")\n",
    "    pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "        base_model_name_or_path,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    print(\"Base pipeline loaded.\")\n",
    "\n",
    "    # Load the textual inversion embeddings for each encoder\n",
    "    for info in embedding_files_templates:\n",
    "        filename = info[\"template\"].format(training_step) # Format filename with the specific step\n",
    "        file_path = os.path.join(local_embeddings_folder, filename)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Embedding file '{filename}' not found in folder '{local_embeddings_folder}'. \"\n",
    "                f\"Expected path: {file_path}\"\n",
    "            )\n",
    "\n",
    "        text_encoder = getattr(pipe, info[\"encoder_attr\"])\n",
    "        tokenizer = getattr(pipe, info[\"tokenizer_attr\"])\n",
    "\n",
    "        print(f\"Loading TI embeddings from '{file_path}' for {info['encoder_attr']}...\")\n",
    "        pipe.load_textual_inversion(\n",
    "            file_path,\n",
    "            token=placeholder_token,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        print(f\"Successfully loaded TI for {info['encoder_attr']}.\")\n",
    "\n",
    "    print(f\"All Textual Inversion embeddings for step {training_step} loaded from local folder.\")\n",
    "    \n",
    "    pipe.to(device)\n",
    "    print(f\"Pipeline moved to {device}.\")\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load TI embeddings for step 1500 from local folder: /mnt/lustre/work/oh/owl661/compositional-vaes/sd3_whatsappA_embedding/wandb-a5a0j11k\n",
      "Using device: cuda\n",
      "Using placeholder token: '<WhatsApp>' for training step: 1500\n",
      "Loading base model 'stabilityai/stable-diffusion-3-medium-diffusers'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c75b6924ccc4f95aa7fc295f6b5376e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7232402b1e7d411b8867319653c83831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base pipeline loaded.\n",
      "Loading TI embeddings from '/mnt/lustre/work/oh/owl661/compositional-vaes/sd3_whatsappA_embedding/wandb-a5a0j11k/learned_embeds_t1-steps-1500.safetensors' for text_encoder...\n",
      "An unexpected error occurred: 'StableDiffusion3Pipeline' object has no attribute 'load_textual_inversion'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3568093/605165500.py\", line 18, in <module>\n",
      "    pipeline = load_pipeline_with_ti_from_local_folder_step(\n",
      "  File \"/tmp/ipykernel_3568093/3024115148.py\", line 75, in load_pipeline_with_ti_from_local_folder_step\n",
      "    pipe.load_textual_inversion(\n",
      "  File \"/mnt/lustre/work/oh/owl661/lib/python3.10/site-packages/diffusers/configuration_utils.py\", line 144, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'StableDiffusion3Pipeline' object has no attribute 'load_textual_inversion'\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# Path to the folder containing the step-specific embeddings\n",
    "my_local_folder_with_embeddings = \"/mnt/lustre/work/oh/owl661/compositional-vaes/sd3_whatsappA_embedding/wandb-a5a0j11k\"\n",
    "base_model = \"stabilityai/stable-diffusion-3-medium-diffusers\" # Or your specific SD3 base model\n",
    "\n",
    "# !!! IMPORTANT: You NEED to provide the placeholder token that was used during training !!!\n",
    "# This information should be known from your training setup.\n",
    "# It's often logged in the W&B run's config or in your training script's arguments.\n",
    "my_placeholder_token = \"<WhatsApp>\" # REPLACE THIS! For example: \"<whatsappA-style>\"\n",
    "\n",
    "chosen_training_step = 1500\n",
    "\n",
    "if my_placeholder_token == \"<YOUR-TRAINING-PLACEHOLDER-TOKEN>\":\n",
    "    print(\"ERROR: Please replace '<YOUR-TRAINING-PLACEHOLDER-TOKEN>' with the actual placeholder token used during training.\")\n",
    "else:\n",
    "    print(f\"Attempting to load TI embeddings for step {chosen_training_step} from local folder: {my_local_folder_with_embeddings}\")\n",
    "    try:\n",
    "        pipeline = load_pipeline_with_ti_from_local_folder_step(\n",
    "            local_embeddings_folder=my_local_folder_with_embeddings,\n",
    "            base_model_name_or_path=base_model,\n",
    "            placeholder_token=my_placeholder_token,\n",
    "            training_step=chosen_training_step,\n",
    "            torch_dtype=torch.float16 # Use bfloat16 if your hardware supports it and you trained with it\n",
    "        )\n",
    "        print(f\"Pipeline with Textual Inversion embeddings for step {chosen_training_step} loaded successfully from local files!\")\n",
    "\n",
    "        # Example prompt using the placeholder token\n",
    "        prompt_template = \"A photo of a cat in the style of {}\"\n",
    "        prompt = prompt_template.format(my_placeholder_token)\n",
    "        print(f\"Generating image with prompt: '{prompt}'\")\n",
    "\n",
    "        # Generate an image\n",
    "        image = pipeline(prompt, num_inference_steps=28, guidance_scale=7.0).images[0]\n",
    "        output_filename = f\"ti_step{chosen_training_step}_loaded_from_local.png\"\n",
    "        image.save(output_filename)\n",
    "        print(f\"Image saved to {output_filename}\")\n",
    "\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f\"File error: {fnfe}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Configuration error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ebeda55a25400f918401bfa1b04a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82788d254dec45fe8d323c9a78edd50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097cc6348b5545bfbb86bf49473b7d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "# pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large-turbo\", torch_dtype=torch.bfloat16)\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large-turbo\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_latent = torch.randn(1, 16, 128, 128, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e2379e36494906be8b1ffed5bc6e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image = pipe(\n",
    "        \"a dog riding a horse in Tubingen\",\n",
    "        num_inference_steps=10,\n",
    "        guidance_scale=0.0,\n",
    "        latents=init_latent,\n",
    "    ).images[0]\n",
    "image.save(\"capybara.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9de4257aaf4dc8b5daacb2663936ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(\n",
    "    \"a dog riding a horse in Tubingen\",\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=1.0,\n",
    "    latents=init_latent,\n",
    ").images[0]\n",
    "image.save(\"capybara2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
