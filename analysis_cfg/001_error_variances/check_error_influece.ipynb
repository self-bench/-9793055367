{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Simple idea: check variances of errors at each timestep. If the variance is high, that step infleunces the error more. Therefore timestep weighting makes sense. \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Simple idea: check variances of errors at each timestep. If the variance is high, that step infleunces the error more. Therefore timestep weighting makes sense. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Simple idea: check variances of errors at each timestep. If the variance is high, that step infleunces the error more. Therefore timestep weighting makes sense. \n",
    "\"\"\"\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "def load_and_evaluate_noises(noise_dir, wandb_run_id, similarity_method='l2'):\n",
    "    \"\"\"\n",
    "    Load and evaluate noises from a specific directory.\n",
    "    First index in selected_indices should be the correct one.\n",
    "    Args:\n",
    "       n noise_dir: Directory containing noise files\n",
    "        wandb_run_id: WandB run ID for saving plots\n",
    "        similarity_method: Either 'l2' or 'cosine' for distance calculation\n",
    "    \"\"\"\n",
    "    # First try to load multiple target gaussian noises\n",
    "    target_noise_paths = sorted(glob.glob(os.path.join(noise_dir, 'target_gaussian_noises_batch*.pt')))\n",
    "    if len(target_noise_paths) > 1:\n",
    "        target_noise_paths = sorted(target_noise_paths, key=lambda x: int(x.split('batch')[-1].split('.')[0]))\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    saved_all_timesteps_for_target = False\n",
    "    \n",
    "    if target_noise_paths:\n",
    "        # Multiple target files found - concatenate them\n",
    "        target_gaussian_noises_list = []\n",
    "        for target_path in target_noise_paths:\n",
    "            target_noise_data = torch.load(target_path)\n",
    "            target_gaussian_noises_list.append(target_noise_data['target_gaussian_noises'])\n",
    "        # Concatenate along batch dimension (dim=1)\n",
    "        target_gaussian_noises = torch.cat(target_gaussian_noises_list, dim=1).to(device)  # shape: [timesteps, total_batches, ...]\n",
    "    else:\n",
    "        # Try legacy single file\n",
    "        target_noise_path = os.path.join(noise_dir, 'target_gaussian_noises.pt')\n",
    "        if not os.path.exists(target_noise_path):\n",
    "            print(f\"No target gaussian noise files found at {noise_dir}\")\n",
    "            return None\n",
    "        target_noise_data = torch.load(target_noise_path)\n",
    "        target_gaussian_noises = target_noise_data['target_gaussian_noises'].to(device) # shape: [timesteps, 1, ...]\n",
    "    \n",
    "    # Get all noise files except target_gaussian_noises*.pt\n",
    "    noise_files = sorted([f for f in glob.glob(os.path.join(noise_dir, \"*.pt\")) \n",
    "                  if not any(x in f for x in [\"target_gaussian_noises.pt\", \"target_gaussian_noises_batch\"])],\n",
    "                  key=lambda x: int(x.split('batch')[-1].split('_')[0]))\n",
    "    \n",
    "    # Load and concatenate all noise files\n",
    "    all_cond_noises = []\n",
    "    all_uncond_noises = []\n",
    "    for noise_file in tqdm(noise_files, desc=f\"Loading noise files from {os.path.basename(noise_dir)}\"):\n",
    "        data = torch.load(noise_file)\n",
    "        all_cond_noises.append(data['conditional_noises'])  # shape: [num_selected, timesteps, batch_size, ...]\n",
    "        all_uncond_noises.append(data['unconditional_noises'])  # [1, timesteps, batch_size, ...]\n",
    "    \n",
    "    # Concatenate along batch dimension\n",
    "    \n",
    "    cond_noises = torch.cat(all_cond_noises, dim=2).to(device)  # shape: [num_selected, timesteps, total_batch_size, ...]\n",
    "    uncond_noises = torch.cat(all_uncond_noises, dim=2).to(device)  # [1, timesteps, total_batch_size, ...]\n",
    "    \n",
    "    if len(cond_noises.shape) == 6:\n",
    "        cond_noises = cond_noises.reshape(cond_noises.shape[0], cond_noises.shape[1], cond_noises.shape[2], -1)\n",
    "        uncond_noises = uncond_noises.reshape(uncond_noises.shape[0], uncond_noises.shape[1], uncond_noises.shape[2], -1)\n",
    "        \n",
    "        target_gaussian_noises = target_gaussian_noises.reshape(target_gaussian_noises.shape[0], target_gaussian_noises.shape[1], -1)\n",
    "        saved_all_timesteps_for_target = True\n",
    "        \n",
    "    # Print shapes for debugging\n",
    "    print(f\"Initial shapes:\")\n",
    "    print(f\"cond_noises: {cond_noises.shape}\")\n",
    "    print(f\"uncond_noises: {uncond_noises.shape}\")\n",
    "    print(f\"target_gaussian_noises: {target_gaussian_noises.shape}\")\n",
    "    \n",
    "    # For each timestep, calculate error variances across all samples\n",
    "    num_timesteps = cond_noises.shape[1]\n",
    "    num_samples = cond_noises.shape[2]\n",
    "    num_classes = cond_noises.shape[0]\n",
    "    \n",
    "    # Store errors for each timestep and sample\n",
    "    true_class_errors = []  # Mean errors for class 0 (true class)\n",
    "    true_class_stds = []    # Standard deviations for class 0\n",
    "    incorrect_class_errors = []  # Mean errors for other classes\n",
    "    incorrect_class_stds = []    # Standard deviations for other classes\n",
    "    accuracies_per_timestep = []  # Track accuracy at each timestep\n",
    "    all_errors = []  # Store all errors for aggregation [timestep, num_classes, batch_size]\n",
    "    per_sample_error_gap = []  # Store error gaps between true and incorrect classes\n",
    "    \n",
    "    # For each timestep\n",
    "    for t in range(num_timesteps):\n",
    "        # Get target noise for this timestep\n",
    "        if saved_all_timesteps_for_target:\n",
    "            target_t = target_gaussian_noises[t]  # [total_batches, ...]\n",
    "        else:\n",
    "            target_t = target_gaussian_noises[t]  # [1, features]\n",
    "            \n",
    "        # Get conditional noise predictions for this timestep\n",
    "        cond_t = cond_noises[:, t]  # [num_selected, total_batch_size, ...]\n",
    "        \n",
    "        print(f\"\\nShapes at timestep {t}:\")\n",
    "        print(f\"cond_t: {cond_t.shape}\")\n",
    "        print(f\"target_t: {target_t.shape}\")\n",
    "        \n",
    "        # Calculate errors for each class prediction\n",
    "        if similarity_method == 'l2':\n",
    "            # L2 distance (lower is better)\n",
    "            if not saved_all_timesteps_for_target:  # If spatial dimensions present\n",
    "                cond_t = cond_t.reshape(cond_t.shape[0], cond_t.shape[1], -1)  # [num_classes, total_batch_size, flattened_features]\n",
    "                target_t = target_t.reshape(target_t.shape[0], -1)  # [total_batch_size, flattened_features]\n",
    "                \n",
    "                print(f\"After reshape:\")\n",
    "                print(f\"cond_t: {cond_t.shape}\")\n",
    "                print(f\"target_t: {target_t.shape}\")\n",
    "                \n",
    "                # Expand target_t to match cond_t's shape for broadcasting\n",
    "                target_t = target_t.unsqueeze(0).expand(num_classes, -1, -1)  # [num_classes, total_batch_size, flattened_features]\n",
    "                \n",
    "                print(f\"After expansion:\")\n",
    "                print(f\"target_t: {target_t.shape}\")\n",
    "            else:\n",
    "                target_t = target_t.unsqueeze(0)\n",
    "            dists = torch.norm(cond_t.to(torch.float32) - target_t.to(torch.float32), p=2, dim=-1)  # [num_classes, total_batch_size]\n",
    "            errors = dists\n",
    "            \n",
    "        else:  # cosine similarity\n",
    "            if len(cond_t.shape) == 3:  # If spatial dimensions present\n",
    "                cond_t = cond_t.reshape(cond_t.shape[0], cond_t.shape[1], -1)  # [num_classes, total_batch_size, flattened_features]\n",
    "                target_t = target_t.reshape(target_t.shape[0], -1)  # [total_batch_size, flattened_features]\n",
    "                \n",
    "                # Expand target_t to match cond_t's shape for broadcasting\n",
    "                target_t = target_t.unsqueeze(0).expand(num_classes, -1, -1)  # [num_classes, total_batch_size, flattened_features]\n",
    "            \n",
    "            cond_t_norm = torch.nn.functional.normalize(cond_t, p=2, dim=-1)\n",
    "            target_t_norm = torch.nn.functional.normalize(target_t, p=2, dim=-1)\n",
    "            similarity = (cond_t_norm * target_t_norm).sum(dim=-1)  # [num_classes, total_batch_size]\n",
    "            errors = -similarity  # Convert to error (lower is better)\n",
    "        \n",
    "        # Store raw errors for aggregation\n",
    "        all_errors.append(errors)\n",
    "        \n",
    "        # Calculate per-timestep accuracy (for visualization)\n",
    "        correct_predictions = (errors[0] < errors[1:].min(dim=0)[0])\n",
    "        accuracies_per_timestep.append(correct_predictions.float().mean().item())\n",
    "        \n",
    "        # Calculate mean and std for true class\n",
    "        true_class_mean = errors[0].mean().item()\n",
    "        true_class_std = errors[0].std().item()\n",
    "        true_class_errors.append(true_class_mean)\n",
    "        true_class_stds.append(true_class_std)\n",
    "        \n",
    "        # Calculate mean and std for incorrect classes\n",
    "        incorrect_mean = errors[1:].mean().item()\n",
    "        incorrect_std = errors[1:].std().item()\n",
    "        incorrect_class_errors.append(incorrect_mean)\n",
    "        incorrect_class_stds.append(incorrect_std)\n",
    "        \n",
    "        # Calculate per-sample error gap between true class and incorrect classes\n",
    "        true_class_errors_per_sample = errors[0]  # [batch_size]\n",
    "        incorrect_class_errors_per_sample = errors[1:].min(dim=0)[0]  # [batch_size]\n",
    "        error_gap = incorrect_class_errors_per_sample - true_class_errors_per_sample  # [batch_size]\n",
    "        per_sample_error_gap.append(error_gap.cpu().numpy())\n",
    "    \n",
    "    # Stack all errors and calculate total accuracy\n",
    "    all_errors = torch.stack(all_errors)\n",
    "    summed_errors = all_errors.sum(dim=0)\n",
    "    correct_predictions_total = (summed_errors[0] < summed_errors[1:].min(dim=0)[0])\n",
    "    mean_accuracy = correct_predictions_total.float().mean().item()\n",
    "    \n",
    "    # Calculate influence of each timestep\n",
    "    influences = []  # Store minimum delta needed for each timestep\n",
    "    influence_stds = []  # Store standard deviation of deltas for each timestep\n",
    "    \n",
    "    # For each timestep\n",
    "    for t in range(num_timesteps):\n",
    "        # Get current timestep and other timesteps contributions\n",
    "        current_timestep = all_errors[t]  # [num_classes, batch_size]\n",
    "        other_timesteps_sum = summed_errors - current_timestep  # [num_classes, batch_size]\n",
    "        \n",
    "        # For each sample, find minimum delta needed\n",
    "        sample_deltas = []\n",
    "        for sample_idx in range(num_samples):\n",
    "            # Get current errors from this timestep and others\n",
    "            other_errors = other_timesteps_sum[:, sample_idx].cpu()  # [num_classes]\n",
    "            current_errors = current_timestep[:, sample_idx].cpu()  # [num_classes]\n",
    "            \n",
    "            # Current prediction based on all timesteps\n",
    "            total_errors = other_errors + current_errors\n",
    "            current_pred = total_errors.argmin().item()\n",
    "            \n",
    "            # Find minimum delta needed to change prediction\n",
    "            min_delta = float('inf')\n",
    "            for target_class in range(num_classes):\n",
    "                if target_class == current_pred:\n",
    "                    continue\n",
    "                \n",
    "                # For target_class to win over current_pred, we need:\n",
    "                # (other_errors[target_class] + (current_errors[target_class] + delta)) < (other_errors[current_pred] + current_errors[current_pred])\n",
    "                # So: delta < (other_errors[current_pred] + current_errors[current_pred]) - (other_errors[target_class] + current_errors[target_class])\n",
    "                margin = (other_errors[current_pred] + current_errors[current_pred]) - (other_errors[target_class] + current_errors[target_class])\n",
    "                \n",
    "                # How much do we need to change current timestep's contribution?\n",
    "                # We need to change it by more than the margin\n",
    "                delta_needed = abs(margin)\n",
    "                min_delta = min(min_delta, delta_needed)\n",
    "            \n",
    "            sample_deltas.append(min_delta)\n",
    "        \n",
    "        # Store average influence and std for this timestep\n",
    "        influences.append(np.mean(sample_deltas))\n",
    "        influence_stds.append(np.std(sample_deltas))\n",
    "    \n",
    "    influences = np.array(influences)\n",
    "    influence_stds = np.array(influence_stds)\n",
    "    \n",
    "    # Convert to numpy arrays for plotting\n",
    "    true_errors = np.array(true_class_errors)\n",
    "    true_stds = np.array(true_class_stds)\n",
    "    incorrect_errors = np.array(incorrect_class_errors)\n",
    "    incorrect_stds = np.array(incorrect_class_stds)\n",
    "    accuracies = np.array(accuracies_per_timestep)\n",
    "    timesteps = np.arange(num_timesteps)\n",
    "    \n",
    "    # Find the matching run info for this wandb_run_id\n",
    "    run_info = next((run for run in run_ids if run['id'] == wandb_run_id), None)\n",
    "    if run_info:\n",
    "        title_info = f\"{run_info['gen']} → {run_info['eval']} ({run_info['type']})\"\n",
    "    else:\n",
    "        title_info = wandb_run_id\n",
    "    \n",
    "    # Create figures directory if it doesn't exist\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "    \n",
    "    # Plot 1: Mean errors with std shading\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create primary axis for errors\n",
    "    ax1 = plt.gca()\n",
    "    # Plot true class errors with std shading\n",
    "    ln1 = ax1.plot(timesteps, true_errors, label='True Class Error', color='blue')\n",
    "    ax1.fill_between(timesteps, true_errors - true_stds, true_errors + true_stds,\n",
    "                     alpha=0.3, color='blue', label='True Class Std')\n",
    "    \n",
    "    # Plot incorrect classes errors with std shading\n",
    "    ln2 = ax1.plot(timesteps, incorrect_errors, label='Incorrect Classes Error', color='red')\n",
    "    ax1.fill_between(timesteps, incorrect_errors - incorrect_stds, incorrect_errors + incorrect_stds,\n",
    "                     alpha=0.3, color='red', label='Incorrect Classes Std')\n",
    "    \n",
    "    ax1.set_xlabel('Timestep')\n",
    "    ax1.set_ylabel(f'Error ({similarity_method})')\n",
    "    ax1.tick_params(axis='y')\n",
    "    \n",
    "    # Create secondary axis for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ln3 = ax2.plot(timesteps, accuracies, label='Per-timestep Accuracy', color='green', linestyle='--')\n",
    "    ax2.set_ylabel('Accuracy', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    \n",
    "    # Combine legends\n",
    "    lns = ln1 + ln2 + ln3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='center right')\n",
    "    \n",
    "    plt.title(f'Error and Accuracy per Timestep\\n{title_info}\\nTotal Accuracy (all timesteps): {mean_accuracy:.3f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f'figures/{wandb_run_id}_errors_and_accuracy.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Standard deviations only\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(timesteps, true_stds, label='True Class Std', color='blue')\n",
    "    plt.plot(timesteps, incorrect_stds, label='Incorrect Classes Std', color='red')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel(f'Standard Deviation of {similarity_method} Error')\n",
    "    plt.title(f'Error Standard Deviations per Timestep\\n{title_info}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'figures/{wandb_run_id}_error_stds.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create correlation plots\n",
    "    # Collect all errors per sample for each timestep\n",
    "    errors_per_sample_timestep = []  # Will be [num_timesteps, num_samples]\n",
    "    for t in range(num_timesteps):\n",
    "        if saved_all_timesteps_for_target:\n",
    "            target_t = target_gaussian_noises[t]\n",
    "        else:\n",
    "            target_t = target_gaussian_noises[t]\n",
    "            \n",
    "        cond_t = cond_noises[:, t]\n",
    "        \n",
    "        if similarity_method == 'l2':\n",
    "            if not saved_all_timesteps_for_target:\n",
    "                cond_t = cond_t.reshape(cond_t.shape[0], cond_t.shape[1], -1)\n",
    "                target_t = target_t.reshape(target_t.shape[0], -1)\n",
    "                target_t = target_t.unsqueeze(0).expand(num_classes, -1, -1)\n",
    "            else:\n",
    "                target_t = target_t.unsqueeze(0)\n",
    "            dists = torch.norm(cond_t.to(torch.float32) - target_t.to(torch.float32), p=2, dim=-1)\n",
    "            errors = dists\n",
    "        else:\n",
    "            if len(cond_t.shape) == 3:\n",
    "                cond_t = cond_t.reshape(cond_t.shape[0], cond_t.shape[1], -1)\n",
    "                target_t = target_t.reshape(target_t.shape[0], -1)\n",
    "                target_t = target_t.unsqueeze(0).expand(num_classes, -1, -1)\n",
    "            \n",
    "            cond_t_norm = torch.nn.functional.normalize(cond_t, p=2, dim=-1)\n",
    "            target_t_norm = torch.nn.functional.normalize(target_t, p=2, dim=-1)\n",
    "            similarity = (cond_t_norm * target_t_norm).sum(dim=-1)\n",
    "            errors = -similarity\n",
    "            \n",
    "        # Store raw errors for correlation analysis\n",
    "        errors_per_sample_timestep.append(errors.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy array [num_timesteps, num_classes, num_samples]\n",
    "    errors_array = np.stack(errors_per_sample_timestep)\n",
    "    \n",
    "    # Calculate correlations for true class (class 0)\n",
    "    true_class_errors_per_sample = errors_array[:, 0, :]  # [num_timesteps, num_samples]\n",
    "    true_class_pearson_corr = np.corrcoef(true_class_errors_per_sample)\n",
    "    \n",
    "    # Calculate Spearman rank correlation for true class\n",
    "    true_class_spearman_corr = np.zeros((num_timesteps, num_timesteps))\n",
    "    for i in range(num_timesteps):\n",
    "        for j in range(num_timesteps):\n",
    "            rho, _ = scipy.stats.spearmanr(true_class_errors_per_sample[i], true_class_errors_per_sample[j])\n",
    "            true_class_spearman_corr[i, j] = rho\n",
    "    \n",
    "    # Calculate correlations for incorrect classes (average across classes 1+)\n",
    "    incorrect_class_errors_per_sample = errors_array[:, 1:, :].mean(axis=1)  # [num_timesteps, num_samples]\n",
    "    incorrect_class_pearson_corr = np.corrcoef(incorrect_class_errors_per_sample)\n",
    "    \n",
    "    # Calculate Spearman rank correlation for incorrect classes\n",
    "    incorrect_class_spearman_corr = np.zeros((num_timesteps, num_timesteps))\n",
    "    for i in range(num_timesteps):\n",
    "        for j in range(num_timesteps):\n",
    "            rho, _ = scipy.stats.spearmanr(incorrect_class_errors_per_sample[i], incorrect_class_errors_per_sample[j])\n",
    "            incorrect_class_spearman_corr[i, j] = rho\n",
    "    \n",
    "    # Plot correlation heatmaps (2x2 grid: Pearson and Spearman for both true and incorrect)\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # True class correlations - Pearson\n",
    "    sns.heatmap(true_class_pearson_corr, ax=ax1, cmap='RdBu_r', vmin=-1, vmax=1,\n",
    "                xticklabels=timesteps[::5], yticklabels=timesteps[::5])\n",
    "    ax1.set_title('True Class Pearson Correlations')\n",
    "    ax1.set_xlabel('Timestep')\n",
    "    ax1.set_ylabel('Timestep')\n",
    "    \n",
    "    # True class correlations - Spearman\n",
    "    sns.heatmap(true_class_spearman_corr, ax=ax2, cmap='RdBu_r', vmin=-1, vmax=1,\n",
    "                xticklabels=timesteps[::5], yticklabels=timesteps[::5])\n",
    "    ax2.set_title('True Class Spearman Rank Correlations')\n",
    "    ax2.set_xlabel('Timestep')\n",
    "    ax2.set_ylabel('Timestep')\n",
    "    \n",
    "    # Incorrect class correlations - Pearson\n",
    "    sns.heatmap(incorrect_class_pearson_corr, ax=ax3, cmap='RdBu_r', vmin=-1, vmax=1,\n",
    "                xticklabels=timesteps[::5], yticklabels=timesteps[::5])\n",
    "    ax3.set_title('Incorrect Classes Pearson Correlations')\n",
    "    ax3.set_xlabel('Timestep')\n",
    "    ax3.set_ylabel('Timestep')\n",
    "    \n",
    "    # Incorrect class correlations - Spearman\n",
    "    sns.heatmap(incorrect_class_spearman_corr, ax=ax4, cmap='RdBu_r', vmin=-1, vmax=1,\n",
    "                xticklabels=timesteps[::5], yticklabels=timesteps[::5])\n",
    "    ax4.set_title('Incorrect Classes Spearman Rank Correlations')\n",
    "    ax4.set_xlabel('Timestep')\n",
    "    ax4.set_ylabel('Timestep')\n",
    "    \n",
    "    plt.suptitle(f'Error Correlations Between Timesteps\\n{title_info}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/{wandb_run_id}_error_correlations.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot influences with std shading\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(timesteps, influences, label='Mean Timestep Influence', color='purple')\n",
    "    plt.fill_between(timesteps, influences - influence_stds, influences + influence_stds,\n",
    "                     alpha=0.3, color='purple', label='Influence Std')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Average Influence (min delta needed)')\n",
    "    plt.title(f'Timestep Influence on Final Prediction\\n{title_info}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'figures/{wandb_run_id}_timestep_influence.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return statistics\n",
    "    return {\n",
    "        'true_errors': true_errors,\n",
    "        'true_stds': true_stds,\n",
    "        'incorrect_errors': incorrect_errors,\n",
    "        'incorrect_stds': incorrect_stds,\n",
    "        'accuracies': accuracies,\n",
    "        'timesteps': timesteps,\n",
    "        'num_samples': num_samples,\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'summed_errors': summed_errors.cpu().numpy(),\n",
    "        'influences': influences,\n",
    "        'influence_stds': influence_stds,\n",
    "        'true_class_pearson_correlations': true_class_pearson_corr,\n",
    "        'true_class_spearman_correlations': true_class_spearman_corr,\n",
    "        'incorrect_class_pearson_correlations': incorrect_class_pearson_corr,\n",
    "        'incorrect_class_spearman_correlations': incorrect_class_spearman_corr,\n",
    "        'per_sample_error_gap': np.array(per_sample_error_gap) , # [num_timesteps, batch_size]\n",
    "        'errors_array': errors_array,\n",
    "    }\n",
    "\n",
    "# Take only SD3 runs of few categories\n",
    "# run_ids = [\n",
    "#     # {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"whatsup_A\", \"id\": \"2zfmbdgb\"},\n",
    "#     # {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"whatsup_B\", \"id\": \"72srd0uj\"},\n",
    "    \n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"position\", \"id\": \"aut3pbwh\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"counting\", \"id\": \"lxu0ohji\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"color_attr\", \"id\": \"8axobt1l\"},\n",
    "#     # # {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"single\", \"id\": \"wwieih3x\"},\n",
    "    \n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"position\", \"id\": \"curxv9va\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"counting\", \"id\": \"9j8saxms\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"color_attr\", \"id\": \"ok41tk28\"},\n",
    "#     # # {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"single\", \"id\": \"4xuy2o92\"},\n",
    "    \n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"counting\", \"id\": \"1tcxc2rz\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"color_attr\", \"id\": \"znbz2uhj\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"position\", \"id\": \"4xuy2o92\"},\n",
    "    \n",
    "# ]\n",
    "\n",
    "\n",
    "run_ids = [\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd2\", \"type\": \"position\", \"id\": \"aut3pbwh\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd2\", \"type\": \"counting\", \"id\": \"lxu0ohji\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd2\", \"type\": \"color_attr\", \"id\": \"8axobt1l\"},\n",
    "    \n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd1.5\", \"type\": \"position\", \"id\": \"curxv9va\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd1.5\", \"type\": \"counting\", \"id\": \"9j8saxms\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd1.5\", \"type\": \"color_attr\", \"id\": \"ok41tk28\"},\n",
    "    \n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd3\", \"type\": \"counting\", \"id\": \"1tcxc2rz\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd3\", \"type\": \"color_attr\", \"id\": \"znbz2uhj\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd3\", \"type\": \"position\", \"id\": \"4xuy2o92\"},\n",
    "    \n",
    "    {\"gen\": \"whatsup-a\", \"eval\": \"sd1.5\", \"type\": \"whatsup-a\", \"id\": \"gtqrvf9c\"},\n",
    "    {\"gen\": \"whatsup-a\", \"eval\": \"sd2\", \"type\": \"whatsup-a\", \"id\": \"qtwnqk5c\"},\n",
    "    {\"gen\": \"whatsup-a\", \"eval\": \"sd3\", \"type\": \"whatsup-a\", \"id\": \"2zfmbdgb\"},\n",
    "    \n",
    "    {\"gen\": \"whatsup-b\", \"eval\": \"sd1.5\", \"type\": \"whatsup-b\", \"id\": \"vqudhuh5\"},\n",
    "    {\"gen\": \"whatsup-b\", \"eval\": \"sd2\", \"type\": \"whatsup-b\", \"id\": \"784a6ywm\"},\n",
    "    {\"gen\": \"whatsup-b\", \"eval\": \"sd3\", \"type\": \"whatsup-b\", \"id\": \"72srd0uj\"},\n",
    "    \n",
    "    {\"gen\": \"sd1.5-cnt\", \"eval\": \"sd1.5\", \"type\": \"sd1.5-cnt\", \"id\": \"h315owa1\"},\n",
    "    {\"gen\": \"sd1.5-cnt\", \"eval\": \"sd2\", \"type\": \"sd1.5-cnt\", \"id\": \"pctg0k6y\"},\n",
    "    {\"gen\": \"sd1.5-cnt\", \"eval\": \"sd3\", \"type\": \"sd1.5-cnt\", \"id\": \"jx3fdoii\"},\n",
    "    \n",
    "    {\"gen\": \"sd2-cnt\", \"eval\": \"sd1.5\", \"type\": \"sd2-cnt\", \"id\": \"xc0afsg5\"},\n",
    "    {\"gen\": \"sd2-cnt\", \"eval\": \"sd2\", \"type\": \"sd2-cnt\", \"id\": \"cmlxwe5a\"},\n",
    "    {\"gen\": \"sd2-cnt\", \"eval\": \"sd3\", \"type\": \"sd2-cnt\", \"id\": \"ygjlsq3o\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating run: aut3pbwh (sd3->sd2, position)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n",
      "Loading noise files from aut3pbwh:   0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from aut3pbwh: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 113, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 113, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 113\n",
      "Mean error across all timesteps: 37.1801\n",
      "Mean std across all timesteps: 37.1849\n",
      "Max std at timestep 0: 87.8117\n",
      "Min std at timestep 29: 7.2873\n",
      "Mean accuracy: 0.372\n",
      "\n",
      "Evaluating run: lxu0ohji (sd3->sd2, counting)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from lxu0ohji:   0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from lxu0ohji: 100%|██████████| 4/4 [00:01<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 230, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 230, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 230\n",
      "Mean error across all timesteps: 34.2766\n",
      "Mean std across all timesteps: 34.3095\n",
      "Max std at timestep 0: 81.6847\n",
      "Min std at timestep 29: 7.0640\n",
      "Mean accuracy: 0.609\n",
      "\n",
      "Evaluating run: 8axobt1l (sd3->sd2, color_attr)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from 8axobt1l:   0%|          | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from 8axobt1l: 100%|██████████| 8/8 [00:01<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 252, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 252, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 252\n",
      "Mean error across all timesteps: 33.2698\n",
      "Mean std across all timesteps: 33.3401\n",
      "Max std at timestep 0: 78.8245\n",
      "Min std at timestep 29: 7.5274\n",
      "Mean accuracy: 0.952\n",
      "\n",
      "Evaluating run: curxv9va (sd3->sd1.5, position)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from curxv9va:   0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from curxv9va: 100%|██████████| 2/2 [00:00<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 113, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 113, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 113, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 113\n",
      "Mean error across all timesteps: 37.2327\n",
      "Mean std across all timesteps: 37.2335\n",
      "Max std at timestep 0: 88.0518\n",
      "Min std at timestep 29: 7.2980\n",
      "Mean accuracy: 0.274\n",
      "\n",
      "Evaluating run: 9j8saxms (sd3->sd1.5, counting)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from 9j8saxms:   0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from 9j8saxms: 100%|██████████| 4/4 [00:01<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 230, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 230, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 230, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 230\n",
      "Mean error across all timesteps: 34.3401\n",
      "Mean std across all timesteps: 34.3637\n",
      "Max std at timestep 0: 81.9747\n",
      "Min std at timestep 29: 7.0496\n",
      "Mean accuracy: 0.609\n",
      "\n",
      "Evaluating run: ok41tk28 (sd3->sd1.5, color_attr)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from ok41tk28:   0%|          | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from ok41tk28: 100%|██████████| 8/8 [00:01<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 252, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 252, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 252, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "Total samples: 252\n",
      "Mean error across all timesteps: 33.3871\n",
      "Mean std across all timesteps: 33.4580\n",
      "Max std at timestep 0: 79.2327\n",
      "Min std at timestep 29: 7.5507\n",
      "Mean accuracy: 0.937\n",
      "\n",
      "Evaluating run: 1tcxc2rz (sd3->sd3, counting)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from 1tcxc2rz:   0%|          | 0/58 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from 1tcxc2rz: 100%|██████████| 58/58 [00:15<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 230, 262144])\n",
      "uncond_noises: torch.Size([1, 30, 230, 262144])\n",
      "target_gaussian_noises: torch.Size([30, 230, 262144])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 230, 262144])\n",
      "target_t: torch.Size([230, 262144])\n",
      "Total samples: 230\n",
      "Mean error across all timesteps: 230.5238\n",
      "Mean std across all timesteps: 232.6176\n",
      "Max std at timestep 29: 418.0072\n",
      "Min std at timestep 12: 179.8112\n",
      "Mean accuracy: 0.930\n",
      "\n",
      "Evaluating run: znbz2uhj (sd3->sd3, color_attr)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from znbz2uhj:   0%|          | 0/63 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from znbz2uhj: 100%|██████████| 63/63 [00:04<00:00, 13.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 252, 65536])\n",
      "uncond_noises: torch.Size([1, 30, 252, 65536])\n",
      "target_gaussian_noises: torch.Size([30, 252, 65536])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 252, 65536])\n",
      "target_t: torch.Size([252, 65536])\n",
      "Total samples: 252\n",
      "Mean error across all timesteps: 133.1846\n",
      "Mean std across all timesteps: 136.5643\n",
      "Max std at timestep 29: 251.6702\n",
      "Min std at timestep 10: 101.9613\n",
      "Mean accuracy: 0.988\n",
      "\n",
      "Evaluating run: 4xuy2o92 (sd3->sd3, position)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from 4xuy2o92:   0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from 4xuy2o92: 100%|██████████| 4/4 [00:01<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 113, 65536])\n",
      "uncond_noises: torch.Size([1, 30, 113, 65536])\n",
      "target_gaussian_noises: torch.Size([30, 113, 65536])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 113, 65536])\n",
      "target_t: torch.Size([113, 65536])\n",
      "Total samples: 113\n",
      "Mean error across all timesteps: 138.6847\n",
      "Mean std across all timesteps: 139.3890\n",
      "Max std at timestep 0: 224.3085\n",
      "Min std at timestep 13: 111.3139\n",
      "Mean accuracy: 0.770\n",
      "\n",
      "Evaluating run: gtqrvf9c (whatsup-a->sd1.5, whatsup-a)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from gtqrvf9c:   0%|          | 0/13 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from gtqrvf9c: 100%|██████████| 13/13 [00:01<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 412, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 412, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 412, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "Total samples: 412\n",
      "Mean error across all timesteps: 31.1873\n",
      "Mean std across all timesteps: 31.1886\n",
      "Max std at timestep 0: 85.7123\n",
      "Min std at timestep 29: 5.8427\n",
      "Mean accuracy: 0.279\n",
      "\n",
      "Evaluating run: qtwnqk5c (whatsup-a->sd2, whatsup-a)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from qtwnqk5c:   0%|          | 0/13 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from qtwnqk5c: 100%|██████████| 13/13 [00:01<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 412, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 412, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 412, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 412, 16384])\n",
      "target_t: torch.Size([412, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 412, 16384])\n",
      "Total samples: 412\n",
      "Mean error across all timesteps: 31.0433\n",
      "Mean std across all timesteps: 31.0458\n",
      "Max std at timestep 0: 85.1534\n",
      "Min std at timestep 29: 5.8412\n",
      "Mean accuracy: 0.262\n",
      "\n",
      "Evaluating run: 2zfmbdgb (whatsup-a->sd3, whatsup-a)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from 2zfmbdgb:   0%|          | 0/26 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from 2zfmbdgb: 100%|██████████| 26/26 [00:06<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 412, 65536])\n",
      "uncond_noises: torch.Size([1, 30, 412, 65536])\n",
      "target_gaussian_noises: torch.Size([30, 412, 65536])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 412, 65536])\n",
      "target_t: torch.Size([412, 65536])\n",
      "Total samples: 412\n",
      "Mean error across all timesteps: 111.4023\n",
      "Mean std across all timesteps: 111.4280\n",
      "Max std at timestep 0: 221.4441\n",
      "Min std at timestep 13: 82.3233\n",
      "Mean accuracy: 0.269\n",
      "\n",
      "Evaluating run: vqudhuh5 (whatsup-b->sd1.5, whatsup-b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from vqudhuh5:   0%|          | 0/13 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from vqudhuh5: 100%|██████████| 13/13 [00:01<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 408, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 408, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 408, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "Total samples: 408\n",
      "Mean error across all timesteps: 30.2247\n",
      "Mean std across all timesteps: 30.2261\n",
      "Max std at timestep 0: 87.9174\n",
      "Min std at timestep 29: 5.7917\n",
      "Mean accuracy: 0.272\n",
      "\n",
      "Evaluating run: 784a6ywm (whatsup-b->sd2, whatsup-b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from 784a6ywm:   0%|          | 0/13 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from 784a6ywm: 100%|██████████| 13/13 [00:01<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 408, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 408, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 408, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 408, 16384])\n",
      "target_t: torch.Size([408, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 408, 16384])\n",
      "Total samples: 408\n",
      "Mean error across all timesteps: 29.9382\n",
      "Mean std across all timesteps: 29.9397\n",
      "Max std at timestep 0: 87.0027\n",
      "Min std at timestep 29: 5.8104\n",
      "Mean accuracy: 0.275\n",
      "\n",
      "Evaluating run: 72srd0uj (whatsup-b->sd3, whatsup-b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from 72srd0uj:   0%|          | 0/26 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from 72srd0uj: 100%|██████████| 26/26 [00:06<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 408, 65536])\n",
      "uncond_noises: torch.Size([1, 30, 408, 65536])\n",
      "target_gaussian_noises: torch.Size([30, 408, 65536])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 408, 65536])\n",
      "target_t: torch.Size([408, 65536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 408\n",
      "Mean error across all timesteps: 103.4205\n",
      "Mean std across all timesteps: 103.4922\n",
      "Max std at timestep 0: 219.2736\n",
      "Min std at timestep 13: 70.5974\n",
      "Mean accuracy: 0.304\n",
      "\n",
      "Evaluating run: h315owa1 (sd1.5-cnt->sd1.5, sd1.5-cnt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from h315owa1:   0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from h315owa1: 100%|██████████| 2/2 [00:00<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 98, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 98, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 98\n",
      "Mean error across all timesteps: 38.9019\n",
      "Mean std across all timesteps: 38.9356\n",
      "Max std at timestep 0: 92.7239\n",
      "Min std at timestep 29: 7.6618\n",
      "Mean accuracy: 0.755\n",
      "\n",
      "Evaluating run: pctg0k6y (sd1.5-cnt->sd2, sd1.5-cnt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from pctg0k6y:   0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from pctg0k6y: 100%|██████████| 2/2 [00:00<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 98, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 98, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 98, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "Total samples: 98\n",
      "Mean error across all timesteps: 39.2084\n",
      "Mean std across all timesteps: 39.2310\n",
      "Max std at timestep 0: 93.3818\n",
      "Min std at timestep 29: 7.6843\n",
      "Mean accuracy: 0.510\n",
      "\n",
      "Evaluating run: jx3fdoii (sd1.5-cnt->sd3, sd1.5-cnt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from jx3fdoii:   0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from jx3fdoii: 100%|██████████| 4/4 [00:01<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 98, 65536])\n",
      "uncond_noises: torch.Size([1, 30, 98, 65536])\n",
      "target_gaussian_noises: torch.Size([30, 98, 65536])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 98, 65536])\n",
      "target_t: torch.Size([98, 65536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 98\n",
      "Mean error across all timesteps: 139.1871\n",
      "Mean std across all timesteps: 139.5909\n",
      "Max std at timestep 0: 233.5324\n",
      "Min std at timestep 13: 109.6694\n",
      "Mean accuracy: 0.327\n",
      "\n",
      "Evaluating run: xc0afsg5 (sd2-cnt->sd1.5, sd2-cnt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from xc0afsg5:   0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from xc0afsg5: 100%|██████████| 2/2 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 111, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 111, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 111, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_noise_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 111\n",
      "Mean error across all timesteps: 38.3425\n",
      "Mean std across all timesteps: 38.3605\n",
      "Max std at timestep 0: 91.7874\n",
      "Min std at timestep 29: 7.7348\n",
      "Mean accuracy: 0.613\n",
      "\n",
      "Evaluating run: cmlxwe5a (sd2-cnt->sd2, sd2-cnt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading noise files from cmlxwe5a:   0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from cmlxwe5a: 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 19, 16384])\n",
      "uncond_noises: torch.Size([1, 30, 19, 16384])\n",
      "target_gaussian_noises: torch.Size([30, 1, 16384])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After reshape:\n",
      "cond_t: torch.Size([4, 19, 16384])\n",
      "target_t: torch.Size([1, 16384])\n",
      "After expansion:\n",
      "target_t: torch.Size([4, 1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 19\n",
      "Mean error across all timesteps: 39.2746\n",
      "Mean std across all timesteps: 39.2955\n",
      "Max std at timestep 0: 93.7110\n",
      "Min std at timestep 29: 7.7130\n",
      "Mean accuracy: 0.842\n",
      "\n",
      "Evaluating run: ygjlsq3o (sd2-cnt->sd3, sd2-cnt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1271557/1750620681.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_noise_data = torch.load(target_path)\n",
      "Loading noise files from ygjlsq3o:   0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_1271557/1750620681.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading noise files from ygjlsq3o: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "cond_noises: torch.Size([4, 30, 111, 65536])\n",
      "uncond_noises: torch.Size([1, 30, 111, 65536])\n",
      "target_gaussian_noises: torch.Size([30, 111, 65536])\n",
      "\n",
      "Shapes at timestep 0:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 1:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 2:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 3:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 4:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 5:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 6:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 7:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 8:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 9:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 10:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 11:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 12:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 13:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 14:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 15:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 16:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 17:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 18:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 19:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 20:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 21:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 22:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 23:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 24:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 25:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 26:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 27:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 28:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "\n",
      "Shapes at timestep 29:\n",
      "cond_t: torch.Size([4, 111, 65536])\n",
      "target_t: torch.Size([111, 65536])\n",
      "Total samples: 111\n",
      "Mean error across all timesteps: 136.4777\n",
      "Mean std across all timesteps: 137.0028\n",
      "Max std at timestep 0: 228.5972\n",
      "Min std at timestep 13: 106.7193\n",
      "Mean accuracy: 0.477\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_aggregate_comparison_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Create aggregate comparison plot\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mcreate_aggregate_comparison_plot\u001b[49m(all_stats, run_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_aggregate_comparison_plot' is not defined"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.base_dir = '/mnt/lustre/work/oh/owl661/compositional-vaes/noise_results'\n",
    "        self.run_id = None\n",
    "        self.similarity = 'l2'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "if args.run_id:\n",
    "    # Find the directory containing this run_id\n",
    "    run_dirs = []\n",
    "    for root, dirs, files in os.walk(args.base_dir):\n",
    "        if args.run_id in dirs:\n",
    "            run_dirs.append(os.path.join(root, args.run_id))\n",
    "    \n",
    "    if not run_dirs:\n",
    "        print(f\"No directory found for run_id: {args.run_id}\")\n",
    "        exit(1)\n",
    "        \n",
    "    # Process the specified run\n",
    "    for run_dir in run_dirs:\n",
    "        print(f\"\\nEvaluating run: {args.run_id}\")\n",
    "        stats = load_and_evaluate_noises(run_dir, args.run_id, args.similarity)\n",
    "        if stats is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nResults for {args.run_id}:\")\n",
    "        print(f\"Total samples: {stats['num_samples']}\")\n",
    "        print(f\"Mean error across all timesteps: {stats['true_errors'].mean():.4f}\")\n",
    "        print(f\"Mean std across all timesteps: {stats['incorrect_errors'].mean():.4f}\")\n",
    "        print(f\"Max std at timestep {stats['timesteps'][stats['incorrect_errors'].argmax()]}: {stats['incorrect_errors'].max():.4f}\")\n",
    "        print(f\"Min std at timestep {stats['timesteps'][stats['incorrect_errors'].argmin()]}: {stats['incorrect_errors'].min():.4f}\")\n",
    "        print(f\"Mean accuracy: {stats['mean_accuracy']:.3f}\")\n",
    "else:\n",
    "    # Process all runs in run_ids list and collect stats\n",
    "    all_stats = {}\n",
    "    for run_info in run_ids:\n",
    "        run_id = run_info['id']\n",
    "        run_dir = None\n",
    "        \n",
    "        # Find the directory for this run_id\n",
    "        for root, dirs, files in os.walk(args.base_dir):\n",
    "            if run_id in dirs:\n",
    "                run_dir = os.path.join(root, run_id)\n",
    "                break\n",
    "        \n",
    "        if run_dir is None:\n",
    "            print(f\"No directory found for run_id: {run_id}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nEvaluating run: {run_id} ({run_info['gen']}->{run_info['eval']}, {run_info['type']})\")\n",
    "        stats = load_and_evaluate_noises(run_dir, run_id, args.similarity)\n",
    "        if stats is None:\n",
    "            continue\n",
    "        \n",
    "        # Store stats for this run\n",
    "        all_stats[run_id] = stats\n",
    "        \n",
    "        print(f\"Total samples: {stats['num_samples']}\")\n",
    "        print(f\"Mean error across all timesteps: {stats['true_errors'].mean():.4f}\")\n",
    "        print(f\"Mean std across all timesteps: {stats['incorrect_errors'].mean():.4f}\")\n",
    "        print(f\"Max std at timestep {stats['timesteps'][stats['incorrect_errors'].argmax()]}: {stats['incorrect_errors'].max():.4f}\")\n",
    "        print(f\"Min std at timestep {stats['timesteps'][stats['incorrect_errors'].argmin()]}: {stats['incorrect_errors'].min():.4f}\")\n",
    "        print(f\"Mean accuracy: {stats['mean_accuracy']:.3f}\")\n",
    "    \n",
    "    # Create aggregate comparison plot\n",
    "    create_aggregate_comparison_plot(all_stats, run_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt stuff - hide top right splines\n",
    "import seaborn as sns\n",
    "sns.reset_defaults()\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "# global sizes of font 8\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['axes.titlesize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "def create_aggregate_comparison_plot(all_stats, run_ids):\n",
    "    \"\"\"Create aggregate plots comparing SD2 and SD3 evaluations for each task type.\"\"\"\n",
    "    task_types = ['counting', 'position', 'color_attr', 'whatsup-a', 'whatsup-b', 'sd1.5-cnt', 'sd2-cnt']\n",
    "    w,h=8,1.5\n",
    "    fig, axes = plt.subplots(1, 7, figsize=(w,h), sharey=True)\n",
    "    \n",
    "    # Get colorblind-friendly colors\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    color1, color2 = colors[0], colors[1]\n",
    "    # Enable right spine for the second y-axis that will show differences\n",
    "    for ax in axes:\n",
    "        ax2 = ax.get_shared_y_axes().get_siblings(ax)[0]  # Get the twin axis\n",
    "        ax2.spines['right'].set_visible(True)\n",
    "    \n",
    "    for task_idx, task_type in enumerate(task_types):\n",
    "        ax = axes[task_idx]\n",
    "        ax2 = ax.twinx()  # Create second y-axis\n",
    "        ax2.spines['right'].set_visible(True)\n",
    "        \n",
    "        # Get runs for this task type\n",
    "        sd2_run = next(run for run in run_ids if run['eval'] == 'sd2' and run['type'] == task_type)\n",
    "        sd3_run = next(run for run in run_ids if run['eval'] == 'sd3' and run['type'] == task_type)\n",
    "        \n",
    "        # Get stats and normalize each model's errors to [0,1] range\n",
    "        sd2_stats = all_stats[sd2_run['id']]\n",
    "        sd3_stats = all_stats[sd3_run['id']]\n",
    "        \n",
    "        # Normalize SD2 errors\n",
    "        sd2_errors = sd2_stats['true_errors']\n",
    "        sd2_stds = sd2_stats['true_stds']\n",
    "        sd2_errors_norm = (sd2_errors - sd2_errors.min()) / (sd2_errors.max() - sd2_errors.min())\n",
    "        sd2_stds_norm = sd2_stds / (sd2_errors.max() - sd2_errors.min())\n",
    "        \n",
    "        # Normalize SD3 errors\n",
    "        sd3_errors = sd3_stats['true_errors']\n",
    "        sd3_stds = sd3_stats['true_stds']\n",
    "        sd3_errors_norm = (sd3_errors - sd3_errors.min()) / (sd3_errors.max() - sd3_errors.min())\n",
    "        sd3_stds_norm = sd3_stds / (sd3_errors.max() - sd3_errors.min())\n",
    "        \n",
    "        # # Plot SD2 evaluation\n",
    "        # ax.plot(sd2_stats['timesteps'] / 30, sd2_errors_norm, \n",
    "        #         label='SD2', color=color1, linestyle='-', linewidth=0.5)\n",
    "        # ax.fill_between(sd2_stats['timesteps'] / 30, \n",
    "        #                sd2_errors_norm - sd2_stds_norm,\n",
    "        #                sd2_errors_norm + sd2_stds_norm,\n",
    "        #                alpha=0.2, color=color1, edgecolor=None)\n",
    "        \n",
    "        # Plot SD3 evaluation\n",
    "        # ax.plot(sd3_stats['timesteps'] / 30, sd3_errors_norm,\n",
    "        #         label='SD3', color=color2, linestyle='-', linewidth=0.5)\n",
    "        # ax.fill_between(sd3_stats['timesteps'] / 30,\n",
    "        #                sd3_errors_norm - sd3_stds_norm,\n",
    "        #                sd3_errors_norm + sd3_stds_norm,\n",
    "        #                alpha=0.2, color=color2, edgecolor=None)\n",
    "        \n",
    "        # Plot mean per-sample error gaps on second y-axis\n",
    "        # Calculate mean and std per timestep across samples\n",
    "        sd3_errors = sd3_stats['per_sample_error_gap']\n",
    "        sd2_errors = sd2_stats['per_sample_error_gap']\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Normalize the error gaps by dividing by max value\n",
    "        sd2_norm = sd2_errors / np.max(np.abs(sd2_errors), axis=1, keepdims=True)\n",
    "        sd3_norm = sd3_errors / np.max(np.abs(sd3_errors), axis=1, keepdims=True)\n",
    "        # Create violin plots at each timestep\n",
    "        positions = sd2_stats['timesteps'] / 30\n",
    "        # for t_idx, t in enumerate(positions):\n",
    "        #     # Create violin plots for SD2\n",
    "        #     df_sd2 = pd.DataFrame({'x': [t]*len(sd2_norm[t_idx]), 'y': sd2_norm[t_idx]})\n",
    "        #     # sns.violinplot(data=df_sd2, x='x', y='y', ax=ax2, color=color1, alpha=0.4)\n",
    "        #     sns.stripplot(data=df_sd2, x='x', y='y', ax=ax2, color=color1, alpha=0.4, size=1, jitter=0.02)\n",
    "            \n",
    "        #     # Create violin plots for SD3\n",
    "        #     df_sd3 = pd.DataFrame({'x': [t]*len(sd3_norm[t_idx]), 'y': sd3_norm[t_idx]})\n",
    "        #     # sns.violinplot(data=df_sd3, x='x', y='y', ax=ax2, color=color2, alpha=0.4)\n",
    "        #     sns.stripplot(data=df_sd3, x='x', y='y', ax=ax2, color=color2, alpha=0.4, size=1, jitter=0.02)\n",
    "        \n",
    "        # Calculate rate of positive errors for each timestep\n",
    "        sd2_err = np.mean(sd2_errors, axis=1)\n",
    "        sd3_err = np.mean(sd3_errors, axis=1)\n",
    "        \n",
    "        sd2_err = sd2_err / (np.abs(sd2_err).max())\n",
    "        sd3_err = sd3_err / (np.abs(sd3_err).max())\n",
    "        \n",
    "        sd2_std=np.std(sd2_errors, axis=1)\n",
    "        sd3_std=np.std(sd3_errors, axis=1)\n",
    "        \n",
    "        # plot with dashed\n",
    "        ax2.plot(positions, sd2_err, color=color1, linestyle='--', linewidth=1, label='SD2')\n",
    "        ax2.plot(positions, sd3_err, color=color2, linestyle='--', linewidth=1, label='SD3')\n",
    "        ax2.set_ylabel('Error')\n",
    "        \n",
    "        # plot stds shading\n",
    "        # ax2.fill_between(positions, sd2_err - sd2_std, sd2_err + sd2_std, color=color1, alpha=0.2)\n",
    "        # ax2.fill_between(positions, sd3_err - sd3_std, sd3_err + sd3_std, color=color2, alpha=0.2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Plot positive error rates\n",
    "        # ax2.plot(positions, sd2_pos_rate, color=color1, linestyle='-', linewidth=1, label='SD2')\n",
    "        # ax2.plot(positions, sd3_pos_rate, color=color2, linestyle='-', linewidth=1, label='SD3')\n",
    "        # ax2.set_ylabel('Rate of Positive Errors')\n",
    "        \n",
    "        ax.set_title(f'{task_type.replace(\"_\", \" \").title()}')\n",
    "        ax.set_xlabel('Timestep')\n",
    "        if task_idx == 0:\n",
    "            ax.set_ylabel('Normalized Error')\n",
    "        if task_idx == 1:\n",
    "            ax.legend(fontsize=7, frameon=False, loc='lower center', bbox_to_anchor=(3.5, -0.74), ncol=2)\n",
    "    \n",
    "        ax.locator_params(axis='x', nbins=4)\n",
    "        ax2.locator_params(axis='x', nbins=4)\n",
    "        ax.locator_params(axis='y', nbins=4)\n",
    "        ax2.locator_params(axis='y', nbins=4)\n",
    "        ax2.yaxis.set_major_locator(plt.MaxNLocator(4))  # Force exactly 4 ticks on ax2 y-axis\n",
    "        plt.rc('xtick', labelsize=5)\n",
    "        plt.rc('ytick', labelsize=5)\n",
    "    # resize to w, h, force\n",
    "    fig.set_size_inches(w, h)\n",
    "    # subplot adjust\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.85, bottom=0.35, wspace=0.7)\n",
    "    plt.savefig('figures/aggregate_comparison.pdf', pad_inches=0)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "create_aggregate_comparison_plot(all_stats, run_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def create_aggregate_comparison_plot_violin(all_stats, run_ids):\n",
    "    \"\"\"Create aggregate plots comparing SD2 and SD3 evaluations for each task type.\"\"\"\n",
    "    task_types = ['counting', 'position', 'color_attr', 'whatsup-a', 'whatsup-b', 'sd1.5-cnt', 'sd2-cnt']\n",
    "    w,h=18,3 # Increased height to accommodate two rows\n",
    "    fig, axes = plt.subplots(2, 7, figsize=(w,h), sharex=True)\n",
    "    \n",
    "    # Get colorblind-friendly colors\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    color1, color2 = colors[0], colors[1]\n",
    "\n",
    "    for task_idx, task_type in enumerate(task_types):\n",
    "        ax_sd2 = axes[0, task_idx]  # Top row for SD2\n",
    "        ax_sd3 = axes[1, task_idx]  # Bottom row for SD3\n",
    "        \n",
    "        # Get runs for this task type\n",
    "        sd2_run = next(run for run in run_ids if run['eval'] == 'sd2' and run['type'] == task_type)\n",
    "        sd3_run = next(run for run in run_ids if run['eval'] == 'sd3' and run['type'] == task_type)\n",
    "        \n",
    "        # Get stats and normalize\n",
    "        sd2_stats = all_stats[sd2_run['id']]\n",
    "        sd3_stats = all_stats[sd3_run['id']]\n",
    "        \n",
    "        # Get error gaps and normalize\n",
    "        sd2_errors = sd2_stats['per_sample_error_gap']\n",
    "        sd3_errors = sd3_stats['per_sample_error_gap']\n",
    "        \n",
    "        # Normalize to [0,1] range per sample\n",
    "        sd2_norm = sd2_errors / np.max(sd2_errors, axis=1, keepdims=True)\n",
    "        sd3_norm = sd3_errors / np.max(sd3_errors, axis=1, keepdims=True)\n",
    "        \n",
    "        # Create density plots at each timestep\n",
    "        timesteps = sd2_stats['timesteps'] / 30\n",
    "        \n",
    "        # Create meshgrid for density plot\n",
    "        X, Y = np.meshgrid(timesteps, np.linspace(0, 1, 100))\n",
    "        Z_sd2 = np.zeros_like(X)\n",
    "        Z_sd3 = np.zeros_like(X)\n",
    "        \n",
    "        # Calculate density for each timestep\n",
    "        for t_idx, _ in enumerate(timesteps):\n",
    "            kernel_sd2 = gaussian_kde(sd2_norm[t_idx])\n",
    "            kernel_sd3 = gaussian_kde(sd3_norm[t_idx])\n",
    "            Z_sd2[:, t_idx] = kernel_sd2(Y[:, t_idx])\n",
    "            Z_sd3[:, t_idx] = kernel_sd3(Y[:, t_idx])\n",
    "        \n",
    "        # Normalize densities\n",
    "        Z_sd2 = Z_sd2 / Z_sd2.max()\n",
    "        Z_sd3 = Z_sd3 / Z_sd3.max()\n",
    "        \n",
    "        # Plot densities\n",
    "        ax_sd2.pcolormesh(X, Y, Z_sd2, cmap='viridis', shading='auto')\n",
    "        ax_sd3.pcolormesh(X, Y, Z_sd3, cmap='viridis', shading='auto')\n",
    "        \n",
    "        # Customize plots\n",
    "        if task_idx == 0:\n",
    "            ax_sd2.set_ylabel('SD2\\nNormalized Range')\n",
    "            ax_sd3.set_ylabel('SD3\\nNormalized Range')\n",
    "        \n",
    "        ax_sd3.set_xlabel('Timestep')\n",
    "        ax_sd2.set_title(f'{task_type.replace(\"_\", \" \").title()}')\n",
    "        \n",
    "        # Set ticks\n",
    "        ax_sd2.locator_params(axis='x', nbins=4)\n",
    "        ax_sd3.locator_params(axis='x', nbins=4)\n",
    "        ax_sd2.locator_params(axis='y', nbins=4)\n",
    "        ax_sd3.locator_params(axis='y', nbins=4)\n",
    "        \n",
    "        plt.rc('xtick', labelsize=5)\n",
    "        plt.rc('ytick', labelsize=5)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, hspace=0.3, wspace=0.7)\n",
    "    plt.savefig('figures/aggregate_comparison_density.pdf', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "create_aggregate_comparison_plot_violin(all_stats, run_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigures/accuracy_comparison.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, pad_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    104\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 106\u001b[0m create_accuracy_comparison_plot(\u001b[43mall_stats\u001b[49m, run_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_stats' is not defined"
     ]
    }
   ],
   "source": [
    "def create_accuracy_comparison_plot(all_stats, run_ids):\n",
    "    \"\"\"Create plots comparing per-timestep and overall accuracy for SD1.5, SD2, and SD3.\"\"\"\n",
    "    task_types = ['counting', 'position', 'color_attr', 'whatsup-a', 'whatsup-b', 'sd1.5-cnt', 'sd2-cnt']\n",
    "    w, h = 7, 3  # Increased height to accommodate second row\n",
    "    fig, axes = plt.subplots(2, 7, figsize=(w, h))\n",
    "    \n",
    "    # Get colorblind-friendly colors\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    color1, color2, color3 = colors[0], colors[1], colors[2]\n",
    "    \n",
    "    for task_idx, task_type in enumerate(task_types):\n",
    "        ax_top = axes[0, task_idx]\n",
    "        ax_bottom = axes[1, task_idx]\n",
    "        \n",
    "        # Get runs for this task type\n",
    "        sd15_run = next(run for run in run_ids if run['eval'] == 'sd1.5' and run['type'] == task_type)\n",
    "        sd2_run = next(run for run in run_ids if run['eval'] == 'sd2' and run['type'] == task_type)\n",
    "        sd3_run = next(run for run in run_ids if run['eval'] == 'sd3' and run['type'] == task_type)\n",
    "        \n",
    "        # Get stats\n",
    "        sd15_stats = all_stats[sd15_run['id']]\n",
    "        sd2_stats = all_stats[sd2_run['id']]\n",
    "        sd3_stats = all_stats[sd3_run['id']]\n",
    "        \n",
    "        # Plot per-timestep accuracies on top axis\n",
    "        ax_top.plot(sd15_stats['timesteps'] / 30, sd15_stats['accuracies'], \n",
    "                label='SD1.5', color=color1, linestyle='-')\n",
    "        ax_top.plot(sd2_stats['timesteps'] / 30, sd2_stats['accuracies'],\n",
    "                label='SD2', color=color2, linestyle='-')\n",
    "        ax_top.plot(sd3_stats['timesteps'] / 30, sd3_stats['accuracies'],\n",
    "                label='SD3', color=color3, linestyle='-')\n",
    "        \n",
    "        # Add horizontal lines for overall accuracies\n",
    "        ax_top.axhline(y=sd15_stats['mean_accuracy'], color=color1, linestyle='--', alpha=0.5)\n",
    "        ax_top.axhline(y=sd2_stats['mean_accuracy'], color=color2, linestyle='--', alpha=0.5)\n",
    "        ax_top.axhline(y=sd3_stats['mean_accuracy'], color=color3, linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Calculate softmax probabilities safely using log-sum-exp trick\n",
    "        def safe_softmax(x, axis=1):\n",
    "            # Subtract max for numerical stability\n",
    "            x_max = np.max(x, axis=axis, keepdims=True)\n",
    "            exp_x = np.exp(x - x_max)\n",
    "            # Add small epsilon to avoid division by zero\n",
    "            return exp_x / (np.sum(exp_x, axis=axis, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Get probabilities for individual timesteps\n",
    "        sd15_probs = safe_softmax(-sd15_stats['errors_array'])\n",
    "        sd2_probs = safe_softmax(-sd2_stats['errors_array'])\n",
    "        sd3_probs = safe_softmax(-sd3_stats['errors_array'])\n",
    "        \n",
    "        # Get mean top-1 probabilities per timestep\n",
    "        sd15_top1 = np.mean(np.max(sd15_probs, axis=1), axis=1)\n",
    "        sd2_top1 = np.mean(np.max(sd2_probs, axis=1), axis=1)\n",
    "        sd3_top1 = np.mean(np.max(sd3_probs, axis=1), axis=1)\n",
    "        \n",
    "        # Plot individual timestep probabilities\n",
    "        ax_bottom.plot(sd15_stats['timesteps'] / 30, sd15_top1,\n",
    "                    color=color1, linestyle='-')\n",
    "        ax_bottom.plot(sd2_stats['timesteps'] / 30, sd2_top1,\n",
    "                    color=color2, linestyle='-')\n",
    "        ax_bottom.plot(sd3_stats['timesteps'] / 30, sd3_top1,\n",
    "                    color=color3, linestyle='-')\n",
    "        \n",
    "        # Add horizontal lines for mean probabilities\n",
    "        # for this, we need to get the actual mean first;)\n",
    "        error_sd15_mean=np.mean(sd15_stats['errors_array'], axis=0)\n",
    "        error_sd2_mean=np.mean(sd2_stats['errors_array'], axis=0)\n",
    "        error_sd3_mean=np.mean(sd3_stats['errors_array'], axis=0)\n",
    "        \n",
    "        sd15_mean_softmax=safe_softmax(-error_sd15_mean, axis=0)\n",
    "        sd2_mean_softmax=safe_softmax(-error_sd2_mean, axis=0)\n",
    "        sd3_mean_softmax=safe_softmax(-error_sd3_mean, axis=0)\n",
    "        \n",
    "        sd15_mean_top1=np.mean(np.max(sd15_mean_softmax, axis=0))\n",
    "        sd2_mean_top1=np.mean(np.max(sd2_mean_softmax, axis=0))\n",
    "        sd3_mean_top1=np.mean(np.max(sd3_mean_softmax, axis=0))\n",
    "        \n",
    "        ax_bottom.axhline(y=sd15_mean_top1, color=color1, linestyle='--', alpha=0.5)\n",
    "        ax_bottom.axhline(y=sd2_mean_top1, color=color2, linestyle='--', alpha=0.5)\n",
    "        ax_bottom.axhline(y=sd3_mean_top1, color=color3, linestyle='--', alpha=0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Formatting\n",
    "        ax_top.set_title(f'{task_type.replace(\"_\", \" \").title()}')\n",
    "        if task_idx == 0:\n",
    "            ax_top.set_ylabel('Accuracy')\n",
    "            ax_bottom.set_ylabel('Top-1 Prob')\n",
    "        if task_idx == 1:\n",
    "            ax_top.legend(fontsize=7, frameon=False)\n",
    "            \n",
    "        ax_bottom.set_xlabel('Timestep')\n",
    "        \n",
    "        # Make y labels small\n",
    "        ax_top.tick_params(axis='y', labelsize=6)\n",
    "        ax_bottom.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    # Adjust figure size and spacing\n",
    "    fig.set_size_inches(w, h)\n",
    "    plt.subplots_adjust(left=0.1, right=0.98, top=0.9, bottom=0.15, wspace=0.7, hspace=0.4)\n",
    "    plt.savefig('figures/accuracy_comparison.pdf', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "create_accuracy_comparison_plot(all_stats, run_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2505, 0.2503, 0.2497, 0.2496])\n"
     ]
    }
   ],
   "source": [
    "arr=torch.tensor([-0.2638195, -0.2645999, -0.2667893, -0.2674649])\n",
    "\n",
    "print(torch.nn.functional.softmax(arr, dim=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only SD3 runs of few categories\n",
    "\n",
    "run_ids = [\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"position\", \"id\": \"aut3pbwh\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"counting\", \"id\": \"lxu0ohji\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"color_attr\", \"id\": \"8axobt1l\"},\n",
    "    \n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"position\", \"id\": \"curxv9va\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"counting\", \"id\": \"9j8saxms\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"color_attr\", \"id\": \"ok41tk28\"},\n",
    "    \n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"position\", \"id\": \"4xuy2o92\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"counting\", \"id\": \"1tcxc2rz\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"color_attr\", \"id\": \"znbz2uhj\"},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
