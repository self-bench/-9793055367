{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Simple idea: check variances of errors at each timestep. If the variance is high, that step infleunces the error more. Therefore timestep weighting makes sense. \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Simple idea: check variances of errors at each timestep. If the variance is high, that step infleunces the error more. Therefore timestep weighting makes sense. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Simple idea: check variances of errors at each timestep. If the variance is high, that step infleunces the error more. Therefore timestep weighting makes sense. \n",
    "\"\"\"\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "def load_and_evaluate_noises(noise_dir, wandb_run_id, similarity_method='l2', use_scores=False):\n",
    "    \"\"\"\n",
    "    Load and evaluate noises/scores from a specific directory.\n",
    "    If use_scores=True, evaluates precomputed scores while also tracking influences.\n",
    "    Otherwise, computes errors from noises using L2 or cosine distance.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Get all relevant files\n",
    "    noise_files = sorted(\n",
    "        [f for f in glob.glob(os.path.join(noise_dir, \"*.pt\")) \n",
    "         if not any(x in f for x in [\"target_gaussian_noises.pt\", \"target_gaussian_noises_batch\"])],\n",
    "        key=lambda x: int(x.split('batch')[-1].split('_')[0])\n",
    "    )\n",
    "\n",
    "    if use_scores:\n",
    "        # Load scores and concatenate\n",
    "        all_scores_with_timestep = []\n",
    "        all_correct_indices = []\n",
    "        all_timesteps = None\n",
    "\n",
    "        for noise_file in tqdm(noise_files, desc=f\"Loading score files from {os.path.basename(noise_dir)}\"):\n",
    "            data = torch.load(noise_file)\n",
    "            scores_with_timestep = data['scores_with_timestep'].numpy()\n",
    "            correct_indices = data['correct_indices']\n",
    "            \n",
    "            all_scores_with_timestep.append(scores_with_timestep)\n",
    "            all_correct_indices.extend(correct_indices)\n",
    "            \n",
    "            if all_timesteps is None:\n",
    "                all_timesteps = data['all_timesteps_used']\n",
    "            else:\n",
    "                assert np.array_equal(all_timesteps, data['all_timesteps_used'])\n",
    "\n",
    "        # Concatenate along batch dimension\n",
    "        scores = np.concatenate(all_scores_with_timestep, axis=2)  # [T, P, total_samples]\n",
    "        correct_indices = np.array(all_correct_indices)\n",
    "        num_timesteps, num_prompts, num_samples = scores.shape\n",
    "\n",
    "        # Track statistics per timestep\n",
    "        true_class_errors = []\n",
    "        true_class_stds = []\n",
    "        incorrect_class_errors = []\n",
    "        incorrect_class_stds = []\n",
    "        accuracies_per_timestep = []\n",
    "        all_errors = []\n",
    "        influences = []\n",
    "        influence_stds = []\n",
    "        per_sample_error_gap = []\n",
    "\n",
    "        # Analyze each timestep\n",
    "        for t in range(num_timesteps):\n",
    "            timestep_scores = scores[t]  # [num_prompts, total_samples]\n",
    "            \n",
    "            # Get errors for correct and incorrect classes\n",
    "            correct_scores = np.array([timestep_scores[correct_indices[i], i] for i in range(num_samples)])\n",
    "            incorrect_scores = []\n",
    "            for i in range(num_samples):\n",
    "                mask = np.ones(num_prompts, dtype=bool)\n",
    "                mask[correct_indices[i]] = False\n",
    "                incorrect_scores.append(timestep_scores[mask, i])\n",
    "            incorrect_scores = np.array(incorrect_scores)\n",
    "\n",
    "            # Calculate statistics\n",
    "            true_class_errors.append(np.mean(correct_scores))\n",
    "            true_class_stds.append(np.std(correct_scores))\n",
    "            incorrect_class_errors.append(np.mean(incorrect_scores))\n",
    "            incorrect_class_stds.append(np.std(incorrect_scores))\n",
    "\n",
    "            # Calculate accuracy\n",
    "            min_scores = np.min(timestep_scores, axis=0)\n",
    "            predictions = np.argmin(timestep_scores, axis=0)\n",
    "            accuracy = np.mean(predictions == correct_indices)\n",
    "            accuracies_per_timestep.append(accuracy)\n",
    "            \n",
    "            # Store errors for influence calculation\n",
    "            all_errors.append(timestep_scores)\n",
    "\n",
    "            # Calculate per-sample error gap\n",
    "            incorrect_min_scores = np.array([np.mean(timestep_scores[mask, i]) for i, mask in \n",
    "                                          enumerate([np.ones(num_prompts, dtype=bool) & (np.arange(num_prompts) != ci) \n",
    "                                                   for ci in correct_indices])])\n",
    "            error_gap = incorrect_min_scores - correct_scores\n",
    "            per_sample_error_gap.append(error_gap)\n",
    "\n",
    "        # Stack all errors and calculate influences\n",
    "        all_errors = np.stack(all_errors)  # [T, P, samples]\n",
    "        summed_errors = np.sum(all_errors, axis=0)  # [P, samples]\n",
    "        \n",
    "        # Calculate influences\n",
    "        for t in range(num_timesteps):\n",
    "            current_timestep = all_errors[t]  # [P, samples]\n",
    "            other_timesteps_sum = summed_errors - current_timestep  # [P, samples]\n",
    "            \n",
    "            sample_deltas = []\n",
    "            for sample_idx in range(num_samples):\n",
    "                other_errors = other_timesteps_sum[:, sample_idx]\n",
    "                current_errors = current_timestep[:, sample_idx]\n",
    "                total_errors = other_errors + current_errors\n",
    "                current_pred = np.argmin(total_errors)\n",
    "                \n",
    "                min_delta = float('inf')\n",
    "                for target_class in range(num_prompts):\n",
    "                    if target_class == current_pred:\n",
    "                        continue\n",
    "                    margin = (other_errors[current_pred] + current_errors[current_pred]) - (other_errors[target_class] + current_errors[target_class])\n",
    "                    delta_needed = abs(margin)\n",
    "                    min_delta = min(min_delta, delta_needed)\n",
    "                sample_deltas.append(min_delta)\n",
    "            \n",
    "            influences.append(np.mean(sample_deltas))\n",
    "            influence_stds.append(np.std(sample_deltas))\n",
    "\n",
    "        # Calculate overall accuracy\n",
    "        avg_scores = np.mean(scores, axis=0)  # [P, samples]\n",
    "        predictions = np.argmin(avg_scores, axis=0)\n",
    "        overall_acc = np.mean(predictions == correct_indices)\n",
    "\n",
    "        return {\n",
    "            'true_errors': np.array(true_class_errors),\n",
    "            'true_stds': np.array(true_class_stds),\n",
    "            'incorrect_errors': np.array(incorrect_class_errors),\n",
    "            'incorrect_stds': np.array(incorrect_class_stds),\n",
    "            'accuracies': np.array(accuracies_per_timestep),\n",
    "            'timesteps': all_timesteps,\n",
    "            'num_samples': num_samples,\n",
    "            'mean_accuracy': overall_acc,\n",
    "            'summed_errors': summed_errors,\n",
    "            'influences': np.array(influences),\n",
    "            'influence_stds': np.array(influence_stds),\n",
    "            'scores_shape': scores.shape,\n",
    "            'per_sample_error_gap': np.array(per_sample_error_gap)  # [num_timesteps, batch_size]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # Original noise-based logic\n",
    "        all_cond_noises = []\n",
    "        all_uncond_noises = []\n",
    "        for noise_file in tqdm(noise_files, desc=f\"Loading noise files from {os.path.basename(noise_dir)}\"):\n",
    "            data = torch.load(noise_file)\n",
    "            all_cond_noises.append(data['conditional_noises'])\n",
    "            all_uncond_noises.append(data['unconditional_noises'])\n",
    "\n",
    "        cond_noises = torch.cat(all_cond_noises, dim=2).to(device)\n",
    "        uncond_noises = torch.cat(all_uncond_noises, dim=2).to(device)\n",
    "\n",
    "        if len(cond_noises.shape) == 6:\n",
    "            cond_noises = cond_noises.reshape(cond_noises.shape[0], cond_noises.shape[1], cond_noises.shape[2], -1)\n",
    "            uncond_noises = uncond_noises.reshape(uncond_noises.shape[0], uncond_noises.shape[1], uncond_noises.shape[2], -1)\n",
    "\n",
    "        print(f\"Initial shapes:\")\n",
    "        print(f\"cond_noises: {cond_noises.shape}\")\n",
    "        print(f\"uncond_noises: {uncond_noises.shape}\")\n",
    "\n",
    "        num_timesteps = cond_noises.shape[1]\n",
    "        num_samples = cond_noises.shape[2]\n",
    "        num_classes = cond_noises.shape[0]\n",
    "\n",
    "        true_class_errors = []\n",
    "        true_class_stds = []\n",
    "        incorrect_class_errors = []\n",
    "        incorrect_class_stds = []\n",
    "        accuracies_per_timestep = []\n",
    "        all_errors = []\n",
    "        per_sample_error_gap = []\n",
    "\n",
    "        for t in range(num_timesteps):\n",
    "            cond_t = cond_noises[:, t]\n",
    "            print(f\"\\nShapes at timestep {t}:\")\n",
    "            print(f\"cond_t: {cond_t.shape}\")\n",
    "\n",
    "            if similarity_method == 'l2':\n",
    "                if len(cond_t.shape) == 3:\n",
    "                    cond_t = cond_t.reshape(cond_t.shape[0], cond_t.shape[1], -1)\n",
    "                dists = torch.norm(cond_t.to(torch.float32), p=2, dim=-1)\n",
    "                errors = dists\n",
    "            else:\n",
    "                if len(cond_t.shape) == 3:\n",
    "                    cond_t = cond_t.reshape(cond_t.shape[0], cond_t.shape[1], -1)\n",
    "                cond_t_norm = torch.nn.functional.normalize(cond_t, p=2, dim=-1)\n",
    "                similarity = (cond_t_norm * cond_t_norm).sum(dim=-1)\n",
    "                errors = -similarity\n",
    "\n",
    "            all_errors.append(errors)\n",
    "            correct_predictions = (errors[0] < errors[1:].min(dim=0)[0])\n",
    "            accuracies_per_timestep.append(correct_predictions.float().mean().item())\n",
    "\n",
    "            true_class_mean = errors[0].mean().item()\n",
    "            true_class_std = errors[0].std().item()\n",
    "            true_class_errors.append(true_class_mean)\n",
    "            true_class_stds.append(true_class_std)\n",
    "\n",
    "            incorrect_mean = errors[1:].mean().item()\n",
    "            incorrect_std = errors[1:].std().item()\n",
    "            incorrect_class_errors.append(incorrect_mean)\n",
    "            incorrect_class_stds.append(incorrect_std)\n",
    "\n",
    "            # Calculate per-sample error gap\n",
    "            true_class_errors_per_sample = errors[0]  # [batch_size]\n",
    "            incorrect_class_errors_per_sample = errors[1:].min(dim=0)[0]  # [batch_size]\n",
    "            error_gap = incorrect_class_errors_per_sample - true_class_errors_per_sample  # [batch_size]\n",
    "            per_sample_error_gap.append(error_gap.cpu().numpy())\n",
    "\n",
    "        all_errors = torch.stack(all_errors)\n",
    "        summed_errors = all_errors.sum(dim=0)\n",
    "        correct_predictions_total = (summed_errors[0] < summed_errors[1:].min(dim=0)[0])\n",
    "        mean_accuracy = correct_predictions_total.float().mean().item()\n",
    "\n",
    "        influences = []\n",
    "        influence_stds = []\n",
    "        for t in range(num_timesteps):\n",
    "            current_timestep = all_errors[t]\n",
    "            other_timesteps_sum = summed_errors - current_timestep\n",
    "\n",
    "            sample_deltas = []\n",
    "            for sample_idx in range(num_samples):\n",
    "                other_errors = other_timesteps_sum[:, sample_idx].cpu()\n",
    "                current_errors = current_timestep[:, sample_idx].cpu()\n",
    "                total_errors = other_errors + current_errors\n",
    "                current_pred = total_errors.argmin().item()\n",
    "\n",
    "                min_delta = float('inf')\n",
    "                for target_class in range(num_classes):\n",
    "                    if target_class == current_pred:\n",
    "                        continue\n",
    "                    margin = ((other_errors[current_pred] + current_errors[current_pred])\n",
    "                            - (other_errors[target_class] + current_errors[target_class]))\n",
    "                    delta_needed = abs(margin)\n",
    "                    min_delta = min(min_delta, delta_needed)\n",
    "                sample_deltas.append(min_delta)\n",
    "\n",
    "            influences.append(np.mean(sample_deltas))\n",
    "            influence_stds.append(np.std(sample_deltas))\n",
    "\n",
    "        return {\n",
    "            'true_errors': np.array(true_class_errors),\n",
    "            'true_stds': np.array(true_class_stds),\n",
    "            'incorrect_errors': np.array(incorrect_class_errors),\n",
    "            'incorrect_stds': np.array(incorrect_class_stds),\n",
    "            'accuracies': np.array(accuracies_per_timestep),\n",
    "            'timesteps': np.arange(num_timesteps),\n",
    "            'num_samples': num_samples,\n",
    "            'mean_accuracy': mean_accuracy,\n",
    "            'summed_errors': summed_errors.cpu().numpy(),\n",
    "            'influences': np.array(influences),\n",
    "            'influence_stds': np.array(influence_stds),\n",
    "            'per_sample_error_gap': np.array(per_sample_error_gap)  # [num_timesteps, batch_size]\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating run: an1a08rx (sd3->sd2, counting)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading score files from an1a08rx:   0%|          | 0/58 [00:00<?, ?it/s]/tmp/ipykernel_2226603/3066612443.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(noise_file)\n",
      "Loading score files from an1a08rx: 100%|██████████| 58/58 [00:00<00:00, 647.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error across all timesteps: 34.6992\n",
      "Mean std across all timesteps: 34.7312\n",
      "Max std at timestep 19.0: 88.6906\n",
      "Min std at timestep 989.0: 6.8932\n",
      "Mean accuracy: 0.639\n",
      "\n",
      "Evaluating run: jyo8eg54 (sd3->sd2, single)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading score files from jyo8eg54: 100%|██████████| 79/79 [00:00<00:00, 481.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error across all timesteps: 35.8252\n",
      "Mean std across all timesteps: 36.1726\n",
      "Max std at timestep 19.0: 92.5875\n",
      "Min std at timestep 989.0: 7.0972\n",
      "Mean accuracy: 0.997\n",
      "\n",
      "Evaluating run: 3me8aw5a (sd3->sd2, two)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading score files from 3me8aw5a: 100%|██████████| 33/33 [00:00<00:00, 1135.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error across all timesteps: 38.2739\n",
      "Mean std across all timesteps: 38.4998\n",
      "Max std at timestep 19.0: 97.0145\n",
      "Min std at timestep 989.0: 7.9158\n",
      "Mean accuracy: 1.000\n",
      "\n",
      "Evaluating run: mst9bl9p (sd2->sd3, counting)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading score files from mst9bl9p: 100%|██████████| 28/28 [00:00<00:00, 1300.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error across all timesteps: 262.9860\n",
      "Mean std across all timesteps: 263.5419\n",
      "Max std at timestep 84.9056625366211: 449.8859\n",
      "Min std at timestep 771.8446044921875: 211.7143\n",
      "Mean accuracy: 0.685\n",
      "\n",
      "Evaluating run: 01dpmapt (sd2->sd3, single)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading score files from 01dpmapt: 100%|██████████| 34/34 [00:00<00:00, 489.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error across all timesteps: 256.2554\n",
      "Mean std across all timesteps: 259.4910\n",
      "Max std at timestep 84.9056625366211: 447.0573\n",
      "Min std at timestep 778.8462524414062: 206.8103\n",
      "Mean accuracy: 0.941\n",
      "\n",
      "Evaluating run: 8cv1l6b6 (sd2->sd3, two)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading score files from 8cv1l6b6: 100%|██████████| 26/26 [00:00<00:00, 442.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error across all timesteps: 255.1764\n",
      "Mean std across all timesteps: 257.0352\n",
      "Max std at timestep 84.9056625366211: 440.5613\n",
      "Min std at timestep 757.4257202148438: 202.5320\n",
      "Mean accuracy: 0.779\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # self.base_dir = '/mnt/lustre/work/oh/owl661/compositional-vaes/noise_results'\n",
    "        self.base_dir = '/mnt/lustre/work/oh/owl661/compositional-vaes/score_results'\n",
    "        self.run_id = None\n",
    "        self.similarity = 'l2'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "if args.run_id:\n",
    "    # Find the directory containing this run_id\n",
    "    run_dirs = []\n",
    "    for root, dirs, files in os.walk(args.base_dir):\n",
    "        if args.run_id in dirs:\n",
    "            run_dirs.append(os.path.join(root, args.run_id))\n",
    "    \n",
    "    if not run_dirs:\n",
    "        print(f\"No directory found for run_id: {args.run_id}\")\n",
    "        exit(1)\n",
    "        \n",
    "    # Process the specified run\n",
    "    for run_dir in run_dirs:\n",
    "        print(f\"\\nEvaluating run: {args.run_id}\")\n",
    "        stats = load_and_evaluate_noises(run_dir, args.run_id, args.similarity)\n",
    "        if stats is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nResults for {args.run_id}:\")\n",
    "        print(f\"Total samples: {stats['num_samples']}\")\n",
    "        print(f\"Mean error across all timesteps: {stats['true_errors'].mean():.4f}\")\n",
    "        print(f\"Mean std across all timesteps: {stats['incorrect_errors'].mean():.4f}\")\n",
    "        print(f\"Max std at timestep {stats['timesteps'][stats['incorrect_errors'].argmax()]}: {stats['incorrect_errors'].max():.4f}\")\n",
    "        print(f\"Min std at timestep {stats['timesteps'][stats['incorrect_errors'].argmin()]}: {stats['incorrect_errors'].min():.4f}\")\n",
    "        print(f\"Mean accuracy: {stats['mean_accuracy']:.3f}\")\n",
    "else:\n",
    "    # Process all runs in run_ids list and collect stats\n",
    "    all_stats = {}\n",
    "    for run_info in run_ids:\n",
    "        run_id = run_info['id']\n",
    "        run_dir = None\n",
    "        \n",
    "        # Find the directory for this run_id\n",
    "        for root, dirs, files in os.walk(args.base_dir):\n",
    "            if run_id in dirs:\n",
    "                run_dir = os.path.join(root, run_id)\n",
    "                break\n",
    "        \n",
    "        if run_dir is None:\n",
    "            print(f\"No directory found for run_id: {run_id}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nEvaluating run: {run_id} ({run_info['gen']}->{run_info['eval']}, {run_info['type']})\")\n",
    "        stats = load_and_evaluate_noises(run_dir, run_id, args.similarity, use_scores=True)\n",
    "        if stats is None:\n",
    "            continue\n",
    "        \n",
    "        # Store stats for this run\n",
    "        all_stats[run_id] = stats\n",
    "        \n",
    "        # print(f\"Total samples: {stats['num_samples']}\")\n",
    "        print(f\"Mean error across all timesteps: {stats['true_errors'].mean():.4f}\")\n",
    "        print(f\"Mean std across all timesteps: {stats['incorrect_errors'].mean():.4f}\")\n",
    "        print(f\"Max std at timestep {stats['timesteps'][stats['incorrect_errors'].argmax()]}: {stats['incorrect_errors'].max():.4f}\")\n",
    "        print(f\"Min std at timestep {stats['timesteps'][stats['incorrect_errors'].argmin()]}: {stats['incorrect_errors'].min():.4f}\")\n",
    "        print(f\"Mean accuracy: {stats['mean_accuracy']:.3f}\")\n",
    "    \n",
    "    # Create aggregate comparison plot\n",
    "    create_aggregate_comparison_plot(all_stats, run_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt stuff - hide top right splines\n",
    "import seaborn as sns\n",
    "sns.reset_defaults()\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "# global sizes of font 8\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['axes.labelsize'] = 8\n",
    "plt.rcParams['axes.titlesize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregate_comparison_plot(all_stats, run_ids):\n",
    "    \"\"\"Create aggregate plots comparing SD2 and SD3 evaluations for each task type.\"\"\"\n",
    "    task_types = ['counting', 'position', 'color_attr']\n",
    "    task_types = ['single', 'two', 'counting']\n",
    "    task_name_replacer = {\n",
    "        'single': 'Single object',\n",
    "        'two': 'Two objects',\n",
    "        'counting': 'Counting',\n",
    "        'position': 'Position',\n",
    "        'color_attr': 'Color attribute',\n",
    "    }\n",
    "    w,h=3.25,0.7\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(w,h), sharey=True)\n",
    "    \n",
    "    # Get colorblind-friendly colors\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    color1, color2 = colors[0], colors[1]\n",
    "    \n",
    "    # Configure splines - only show x-axis at 0\n",
    "    for ax in axes:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_position(('data', 0))\n",
    "        ax.spines['bottom'].set_color('black')\n",
    "        ax.spines['bottom'].set_linewidth(0.5)\n",
    "        # Hide overlapping ticks\n",
    "        ax.tick_params(axis='both', length=0, width=0.5)\n",
    "        # Only show 0 on y-axis\n",
    "        ax.set_yticks([0])\n",
    "    \n",
    "    for task_idx, task_type in enumerate(task_types):\n",
    "        ax = axes[task_idx]\n",
    "        \n",
    "        # Get runs for this task type\n",
    "        sd2_run = next(run for run in run_ids if run['eval'] == 'sd2' and run['type'] == task_type)\n",
    "        sd3_run = next(run for run in run_ids if run['eval'] == 'sd3' and run['type'] == task_type)\n",
    "        \n",
    "        # Get stats\n",
    "        sd2_stats = all_stats[sd2_run['id']]\n",
    "        sd3_stats = all_stats[sd3_run['id']]\n",
    "\n",
    "        # Normalize errors by max absolute value\n",
    "        max_abs_error_sd2=np.max(np.abs(sd2_stats['per_sample_error_gap']))\n",
    "        max_abs_error_sd3=np.max(np.abs(sd3_stats['per_sample_error_gap'])) \n",
    "        \n",
    "        # Plot SD2 normalized errors\n",
    "        ax.plot(np.array(sd2_stats['timesteps']) / 1000, np.array(sd2_stats['per_sample_error_gap'].mean(axis=1)  ) / max_abs_error_sd2,\n",
    "                color=color1, linestyle='-', linewidth=1, label='SD2', zorder=10)\n",
    "        ax.fill_between(np.array(sd2_stats['timesteps']) / 1000 ,\n",
    "                       (np.array(sd2_stats['per_sample_error_gap'].mean(axis=1)) - np.array(sd2_stats['per_sample_error_gap'].std(axis=1)   )) / max_abs_error_sd2,\n",
    "                       (np.array(sd2_stats['per_sample_error_gap'].mean(axis=1)) + np.array(sd2_stats['per_sample_error_gap'].std(axis=1)   )) / max_abs_error_sd2,\n",
    "                       alpha=0.1, color=color1, edgecolor=None)\n",
    "        \n",
    "        # Plot SD3 normalized errors\n",
    "        ax.plot(np.array(sd3_stats['timesteps']) / 1000, np.array(sd3_stats['per_sample_error_gap'].mean(axis=1)  ) / max_abs_error_sd3 ,\n",
    "                color=color2, linestyle='-', linewidth=1, label='SD3', zorder=10)\n",
    "        ax.fill_between(np.array(sd3_stats['timesteps'])     / 1000,\n",
    "                       (np.array(sd3_stats['per_sample_error_gap'].mean(axis=1)) - np.array(sd3_stats['per_sample_error_gap'].std(axis=1))   ) / max_abs_error_sd3,\n",
    "                       (np.array(sd3_stats['per_sample_error_gap'].mean(axis=1)) + np.array(sd3_stats['per_sample_error_gap'].std(axis=1))   ) / max_abs_error_sd3,\n",
    "                       alpha=0.1, color=color2, edgecolor=None)\n",
    "        \n",
    "        ax.set_title(f'{task_name_replacer[task_type]}', fontsize=7, pad=-20)\n",
    "        if task_idx == 0:\n",
    "            ax.set_ylabel('Error\\ngap$_{\\\\text{pos}-\\\\text{neg}}$', fontsize=5, labelpad=1)\n",
    "        # if task_idx == 1:\n",
    "        #     ax.legend(fontsize=6, frameon=False, loc='lower center', bbox_to_anchor=(0.5, -0.5), ncol=2)\n",
    "        ax.set_xlabel('Timestep', fontsize=5, labelpad=-5)\n",
    "\n",
    "        ax.locator_params(axis='x', nbins=2)\n",
    "        \n",
    "        ax.tick_params(axis='x', labelsize=4)\n",
    "        ax.tick_params(axis='y', labelsize=4)\n",
    "\n",
    "    # resize to w, h, force\n",
    "    fig.set_size_inches(w, h)\n",
    "    # subplot adjust\n",
    "    plt.subplots_adjust(left=0.08, right=0.98, top=0.87, bottom=0.0, wspace=0.1)\n",
    "    plt.savefig('figures/aggregate_comparison.pdf', pad_inches=0)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_aggregate_comparison_plot(all_stats, run_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_accuracy_comparison_plot(all_stats, run_ids):\n",
    "    \"\"\"Create plots comparing per-timestep and overall accuracy for SD1.5, SD2, and SD3.\"\"\"\n",
    "    task_types = ['counting', 'position', 'color_attr']\n",
    "    task_types = ['single', 'two', 'counting']\n",
    "    w, h = 3.25, 0.75 # Increased height to accommodate second row\n",
    "    show_softmax = False\n",
    "    \n",
    "    task_name_replacer = {\n",
    "        'single': 'Single object',\n",
    "        'two': 'Two objects',\n",
    "        'counting': 'Counting',\n",
    "        'position': 'Position',\n",
    "        'color_attr': 'Color attribute',\n",
    "    }\n",
    "    \n",
    "    if show_softmax:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(w, h))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(w, h), sharey=True)\n",
    "        axes = axes[np.newaxis, :]  # Add dimension to match 2D case\n",
    "    \n",
    "    # Get colorblind-friendly colors\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    color_sd15, color_sd2, color_sd3 = colors[2], colors[0], colors[1]\n",
    "    for ax in axes.flatten():\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        # ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_position(('data', 0))\n",
    "        # ax.spines['bottom'].set_color('black')\n",
    "        ax.spines['bottom'].set_linewidth(0.5)\n",
    "        ax.spines['left'].set_linewidth(0.5)\n",
    "        # Hide overlapping ticks\n",
    "        # ax.tick_params(axis='both', length=0, width=0.5)\n",
    "        # Only show 0 on y-axis\n",
    "        # ax.set_yticks([0])\n",
    "    # Make tick marks shorter\n",
    "    for ax in axes.flatten():\n",
    "        ax.tick_params(axis='both', length=2, width=0.5, labelsize=4)\n",
    "    \n",
    "    \n",
    "    for task_idx, task_type in enumerate(task_types):\n",
    "        ax_top = axes[0, task_idx]\n",
    "        \n",
    "        # Get runs for this task type\n",
    "        # sd15_run = next(run for run in run_ids if run['eval'] == 'sd1.5' and run['type'] == task_type)\n",
    "        sd2_run = next(run for run in run_ids if run['eval'] == 'sd2' and run['type'] == task_type)\n",
    "        sd3_run = next(run for run in run_ids if run['eval'] == 'sd3' and run['type'] == task_type)\n",
    "        \n",
    "        # Get stats\n",
    "        # sd15_stats = all_stats[sd15_run['id']]\n",
    "        sd2_stats = all_stats[sd2_run['id']]\n",
    "        sd3_stats = all_stats[sd3_run['id']]\n",
    "        \n",
    "        # Plot per-timestep accuracies on top axis\n",
    "        # ax_top.plot(sd15_stats['timesteps'] / 30, sd15_stats['accuracies'], \n",
    "        #         label='SD1.5', color=color_sd15, linestyle='-', linewidth=1)\n",
    "        ax_top.plot(np.array(sd2_stats['timesteps']) / 1000, np.array(sd2_stats['accuracies'])    ,\n",
    "                label='SD2 model, SD2 generations', color=color_sd2, linestyle='-', linewidth=1)\n",
    "        ax_top.plot(np.array(sd3_stats['timesteps']) / 1000, np.array(sd3_stats['accuracies']),\n",
    "                label='SD3 model, SD3 generations', color=color_sd3, linestyle='-', linewidth=1)\n",
    "        \n",
    "        # Add horizontal lines for overall accuracies\n",
    "        # ax_top.axhline(y=sd15_stats['mean_accuracy'], color=color_sd15, linestyle='--', alpha=0.5)\n",
    "        # ax_top.axhline(y=sd2_stats['mean_accuracy'], color=color_sd2, linestyle='--', alpha=0.5)\n",
    "        # ax_top.axhline(y=sd3_stats['mean_accuracy'], color=color_sd3, linestyle='--', alpha=0.5)\n",
    "\n",
    "        if show_softmax:\n",
    "            ax_bottom = axes[1, task_idx]\n",
    "            \n",
    "            # Calculate softmax probabilities safely using log-sum-exp trick\n",
    "            def safe_softmax(x, axis=1):\n",
    "                # Subtract max for numerical stability\n",
    "                x_max = np.max(x, axis=axis, keepdims=True)\n",
    "                exp_x = np.exp(x - x_max)\n",
    "                # Add small epsilon to avoid division by zero\n",
    "                return exp_x / (np.sum(exp_x, axis=axis, keepdims=True) + 1e-10)\n",
    "            \n",
    "            # Get probabilities for individual timesteps\n",
    "            # sd15_probs = safe_softmax(-sd15_stats['errors_array'])\n",
    "            sd2_probs = safe_softmax(-sd2_stats['errors_array'])\n",
    "            sd3_probs = safe_softmax(-sd3_stats['errors_array'])\n",
    "            \n",
    "            # Get mean top-1 probabilities per timestep\n",
    "            # sd15_top1 = np.mean(np.max(sd15_probs, axis=1), axis=1)\n",
    "            sd2_top1 = np.mean(np.max(sd2_probs, axis=1), axis=1)\n",
    "            sd3_top1 = np.mean(np.max(sd3_probs, axis=1), axis=1)\n",
    "            \n",
    "            # Plot individual timestep probabilities\n",
    "            # ax_bottom.plot(sd15_stats['timesteps'] / 30, sd15_top1,\n",
    "            #             color=color_sd15, linestyle='-', linewidth=1)\n",
    "            ax_bottom.plot(sd2_stats['timesteps'] / 100, sd2_top1,\n",
    "                        color=color_sd2, linestyle='-', linewidth=1)\n",
    "            ax_bottom.plot(sd3_stats['timesteps'] / 100, sd3_top1,\n",
    "                        color=color_sd3, linestyle='-', linewidth=1)\n",
    "            \n",
    "            # Add horizontal lines for mean probabilities\n",
    "            # error_sd  15_mean = np.mean(sd15_stats['errors_array'], axis=0)\n",
    "            error_sd2_mean = np.mean(sd2_stats['errors_array'], axis=0)\n",
    "            error_sd3_mean = np.mean(sd3_stats['errors_array'], axis=0)\n",
    "            \n",
    "            # sd15_mean_softmax = safe_softmax(-error_sd15_mean, axis=0)\n",
    "            sd2_mean_softmax = safe_softmax(-error_sd2_mean, axis=0)\n",
    "            sd3_mean_softmax = safe_softmax(-error_sd3_mean, axis=0)\n",
    "            \n",
    "            # sd15_mean_top1 = np.mean(np.max(sd15_mean_softmax, axis=0))\n",
    "            sd2_mean_top1 = np.mean(np.max(sd2_mean_softmax, axis=0))\n",
    "            sd3_mean_top1 = np.mean(np.max(sd3_mean_softmax, axis=0))\n",
    "            \n",
    "            ax_bottom.set_xlabel('Timestep', fontsize=5)\n",
    "            if task_idx == 0:\n",
    "                ax_bottom.set_ylabel('Top-1 Prob', fontsize=6)\n",
    "            ax_bottom.tick_params(axis='y', labelsize=4)\n",
    "        # set tick params to small\n",
    "        ax_top.tick_params(axis='x', labelsize=4)\n",
    "        ax_top.tick_params(axis='y', labelsize=4)\n",
    "        \n",
    "        # Formatting\n",
    "        ax_top.set_title(f'{task_name_replacer[task_type]}', fontsize=7, pad=-20)\n",
    "        if task_idx == 0:\n",
    "            ax_top.set_ylabel('Accuracy', fontsize=5, labelpad=0)\n",
    "            \n",
    "        if not show_softmax:\n",
    "            ax_top.set_xlabel('Timestep', fontsize=5, labelpad=0)\n",
    "        \n",
    "        # Make y labels small\n",
    "        ax_top.tick_params(axis='y', labelsize=4)\n",
    "        if task_idx == 0:\n",
    "            # Create legend below the plots\n",
    "            if show_softmax:\n",
    "                fig.legend(fontsize=6, frameon=False, loc='lower center', bbox_to_anchor=(0.5, 0.0), ncol=3)\n",
    "            else:\n",
    "                fig.legend(fontsize=6, frameon=False, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    # y grid\n",
    "    for ax in axes.flatten():\n",
    "        ax.grid(which='major', axis='y', linestyle='-', alpha=0.2)\n",
    "    # Adjust figure size and spacing\n",
    "    fig.set_size_inches(w, h)\n",
    "    if show_softmax:\n",
    "        plt.subplots_adjust(left=0.15, right=0.98, top=0.9, bottom=0.25, wspace=0.7, hspace=0.4)\n",
    "    else:\n",
    "        # plt.subplots_adjust(left=0.14, right=0.98, top=0.85, bottom=0.4, wspace=0.1)\n",
    "        plt.subplots_adjust(left=0.08, right=0.98, top=0.87, bottom=0.4, wspace=0.1)\n",
    "\n",
    "    plt.savefig('figures/accuracy_comparison.pdf', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "create_accuracy_comparison_plot(all_stats, run_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gen': 'sd2', 'eval': 'sd2', 'type': 'counting', 'id': 'rf5u3pz9'}, {'gen': 'sd2', 'eval': 'sd2', 'type': 'single', 'id': 'pjyhjvyw'}, {'gen': 'sd2', 'eval': 'sd2', 'type': 'two', 'id': '3me8aw5a'}, {'gen': 'sd3', 'eval': 'sd3', 'type': 'counting', 'id': 'mst9bl9p'}, {'gen': 'sd3', 'eval': 'sd3', 'type': 'single', 'id': 'dvxgc18d'}, {'gen': 'sd3', 'eval': 'sd3', 'type': 'two', 'id': 'ioer1zo7'}]\n"
     ]
    }
   ],
   "source": [
    "print(run_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2505, 0.2503, 0.2497, 0.2496])\n"
     ]
    }
   ],
   "source": [
    "arr=torch.tensor([-0.2638195, -0.2645999, -0.2667893, -0.2674649])\n",
    "\n",
    "print(torch.nn.functional.softmax(arr, dim=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only SD3 runs of few categories\n",
    "\n",
    "run_ids = [\n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"position\", \"id\": \"aut3pbwh\"},\n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"counting\", \"id\": \"lxu0ohji\"},\n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"color_attr\", \"id\": \"8axobt1l\"},\n",
    "    \n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"position\", \"id\": \"curxv9va\"},\n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"counting\", \"id\": \"9j8saxms\"},\n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd1.5\", \"type\": \"color_attr\", \"id\": \"ok41tk28\"},\n",
    "    \n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"position\", \"id\": \"4xuy2o92\"},\n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"counting\", \"id\": \"1tcxc2rz\"},\n",
    "    # {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"color_attr\", \"id\": \"znbz2uhj\"},\n",
    "    \n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"counting\", \"id\": \"an1a08rx\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"single\", \"id\": \"jyo8eg54\"},\n",
    "    {\"gen\": \"sd3\", \"eval\":\"sd2\", \"type\": \"two\", \"id\": \"3me8aw5a\"},\n",
    "    \n",
    "    {\"gen\": \"sd2\", \"eval\":\"sd3\", \"type\": \"counting\", \"id\": \"mst9bl9p\"},\n",
    "    {\"gen\": \"sd2\", \"eval\":\"sd3\", \"type\": \"single\", \"id\": \"01dpmapt\"},\n",
    "    {\"gen\": \"sd2\", \"eval\":\"sd3\", \"type\": \"two\", \"id\": \"8cv1l6b6\"},\n",
    "]\n",
    "\n",
    "#self domani\n",
    "# run_ids  = [\n",
    "#     {\"gen\": \"sd2\", \"eval\":\"sd2\", \"type\": \"counting\", \"id\": \"rf5u3pz9\"},\n",
    "#     {\"gen\": \"sd2\", \"eval\":\"sd2\", \"type\": \"single\", \"id\": \"pjyhjvyw\"},\n",
    "#     {\"gen\": \"sd2\", \"eval\":\"sd2\", \"type\": \"two\", \"id\": \"3me8aw5a\"},\n",
    "    \n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"counting\", \"id\": \"mst9bl9p\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"single\", \"id\": \"dvxgc18d\"},\n",
    "#     {\"gen\": \"sd3\", \"eval\":\"sd3\", \"type\": \"two\", \"id\": \"ioer1zo7\"},\n",
    "# ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
