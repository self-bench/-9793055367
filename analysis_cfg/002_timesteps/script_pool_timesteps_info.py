"""Script to visualize learned timestep weights across different models and task types.
Loads results from pickle files generated by learn_timestep_weights.py and creates comparison plots.
"""
import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec

# Base path for processed results
RESULTS_DIR = '/mnt/lustre/work/oh/owl661/compositional-vaes/src/vqvae/_post/self_bench/analysis_cfg/processed_runs'

# Define the run configurations we want to analyze
run_ids = [
    {"gen": "sd3", "eval": "sd2", "type": "position", "id": "aut3pbwh"},
    {"gen": "sd3", "eval": "sd2", "type": "counting", "id": "lxu0ohji"},
    {"gen": "sd3", "eval": "sd2", "type": "color_attr", "id": "8axobt1l"},
    
    {"gen": "sd3", "eval": "sd1.5", "type": "position", "id": "curxv9va"},
    {"gen": "sd3", "eval": "sd1.5", "type": "counting", "id": "9j8saxms"},
    {"gen": "sd3", "eval": "sd1.5", "type": "color_attr", "id": "ok41tk28"},
    
    {"gen": "sd3", "eval": "sd3", "type": "counting", "id": "1tcxc2rz"},
    {"gen": "sd3", "eval": "sd3", "type": "color_attr", "id": "znbz2uhj"},
    {"gen": "sd3", "eval": "sd3", "type": "position", "id": "4xuy2o92"},
]

def load_all_results():
    """Preload all results into a dictionary."""
    results_cache = {}
    for run in run_ids:
        results_file = os.path.join(RESULTS_DIR, f'{run["id"]}_results.pkl')
        if not os.path.exists(results_file):
            print(f"Warning: Results file not found for run {run['id']}")
            continue
        
        with open(results_file, 'rb') as f:
            results_cache[run['id']] = pickle.load(f)
    
    return results_cache

def create_comparison_plots(results_cache):
    """Create plots with timestep weights and accuracy bars."""
    # Set up the plot style
    plt.style.use('seaborn-v0_8-paper')
    w, h = 5, 2.5
    
    # Create figure with GridSpec for layout control
    fig = plt.figure(figsize=(w, h))
    gs = GridSpec(2, 3, height_ratios=[2, 1], hspace=0.3)
    
    # Define colors for each model
    model_styles = {
        'sd1.5': {'color': sns.color_palette("colorblind")[0], 'linestyle': '-'},
        'sd2': {'color': sns.color_palette("colorblind")[1], 'linestyle': '-'},
        'sd3': {'color': sns.color_palette("colorblind")[2], 'linestyle': '-'}
    }
    
    task_types = ['position', 'counting', 'color_attr']
    models = ['sd1.5', 'sd2', 'sd3']
    
    # Create plots
    for i, task_type in enumerate(task_types):
        # Top subplot for weights
        ax_top = fig.add_subplot(gs[0, i])
        # Bottom subplot for accuracies
        ax_bottom = fig.add_subplot(gs[1, i])
        
        accuracies = {'uniform': [], 'learned': []}
        
        # Plot weights and collect accuracies
        for model in models:
            relevant_runs = [run for run in run_ids if run['type'] == task_type and run['eval'] == model]
            
            if not relevant_runs:
                continue
            
            for run in relevant_runs:
                if run['id'] not in results_cache:
                    continue
                
                results = results_cache[run['id']]
                
                # Plot weights
                weights = results['timestep_weights']
                weights = weights / weights.max()
                timesteps = np.arange(len(weights))
                
                ax_top.plot(timesteps / len(timesteps), weights, 
                          color=model_styles[model]['color'],
                          linestyle=model_styles[model]['linestyle'],
                          alpha=0.8,
                          label=f"{model.upper()}")
                
                # Store accuracies
                accuracies['uniform'].append((model, results['accuracy_original']))
                accuracies['learned'].append((model, results['accuracy_best']))
        
        # Customize weight plot
        ax_top.set_title(f'{task_type.replace("_", " ").title()}')
        if i == 0:
            ax_top.set_ylabel('Weight')
        ax_top.set_xlim(0, 1)
        ax_top.set_ylim(0, 1)
        if i == 1:
            ax_top.legend(fontsize=6, frameon=False)
        ax_top.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))
        ax_top.yaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))
        
        # Plot accuracy bars
        x = np.arange(len(models))
        width = 0.35
        
        uniform_accs = [acc for model, acc in sorted(accuracies['uniform'], key=lambda x: x[0])]
        learned_accs = [acc for model, acc in sorted(accuracies['learned'], key=lambda x: x[0])]
        
        ax_bottom.bar(x - width/2, uniform_accs, width, label='Uniform', 
                     color='lightgray', alpha=0.5)
        ax_bottom.bar(x + width/2, learned_accs, width, label='Learned',
                     color='gray', alpha=0.5)
        
        # Customize accuracy plot
        ax_bottom.set_xticks(x)
        ax_bottom.set_xticklabels(['1.5', '2', '3'], fontsize=6)
        if i == 0:
            ax_bottom.set_ylabel('Acc.')
        ax_bottom.set_ylim(0, 1)
        ax_bottom.yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))
        if i == 1:
            ax_bottom.legend(fontsize=6, frameon=False, ncol=2)
        
        # Remove top subplot's bottom ticks
        ax_top.set_xticks([])
    
    # Adjust layout
    plt.subplots_adjust(left=0.15, right=0.97, top=0.9, bottom=0.2)
    plt.gcf().set_size_inches(w, h)
    
    # Save plot
    os.makedirs('figures', exist_ok=True)
    plt.savefig('figures/timestep_weights_comparison.pdf', pad_inches=0, dpi=300)
    plt.close()

def create_accuracy_summary(results_cache):
    """Create a summary table of accuracies before and after weighting."""
    print("\nAccuracy Summary:")
    print("-" * 80)
    print(f"{'Task Type':<12} {'Model':<8} {'Original':<10} {'Weighted':<10} {'Improvement':<12}")
    print("-" * 80)
    
    # Store results for averaging
    results_by_type = {}
    
    for task_type in ['position', 'counting', 'color_attr']:
        results_by_type[task_type] = []
        for model in ['sd1.5', 'sd2', 'sd3']:
            runs = [run for run in run_ids if run['type'] == task_type and run['eval'] == model]
            for run in runs:
                if run['id'] not in results_cache:
                    continue
                
                results = results_cache[run['id']]
                orig_acc = results['accuracy_original']
                best_acc = results['accuracy_best']
                improvement = (best_acc - orig_acc) * 100
                
                results_by_type[task_type].append({
                    'model': model,
                    'orig_acc': orig_acc,
                    'best_acc': best_acc,
                    'improvement': improvement
                })
                
                print(f"{task_type:<12} {model:<8} {orig_acc:.3f}     {best_acc:.3f}     {improvement:+.1f}%")
    
    print("-" * 80)
    print("Averages by task type:")
    for task_type, results in results_by_type.items():
        if results:
            avg_improvement = np.mean([r['improvement'] for r in results])
            avg_orig = np.mean([r['orig_acc'] for r in results])
            avg_best = np.mean([r['best_acc'] for r in results])
            print(f"{task_type:<12} {'AVG':<8} {avg_orig:.3f}     {avg_best:.3f}     {avg_improvement:+.1f}%")
    print("-" * 80)

if __name__ == "__main__":
    # Load all results once
    results_cache = load_all_results()
    
    # Create visualizations using cached results
    create_comparison_plots(results_cache)
    create_accuracy_summary(results_cache)

