{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script to visualize learned timestep weights across different models and task types.\n",
    "Loads results from pickle files generated by learn_timestep_weights.py and creates comparison plots.\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Base path for processed results\n",
    "RESULTS_DIR = '/mnt/lustre/work/oh/owl661/compositional-vaes/src/vqvae/_post/self_bench/analysis_cfg/processed_runs'\n",
    "\n",
    "# Define the run configurations we want to analyze\n",
    "run_ids = [\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd2\", \"type\": \"position\", \"id\": \"aut3pbwh\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd2\", \"type\": \"counting\", \"id\": \"lxu0ohji\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd2\", \"type\": \"color_attr\", \"id\": \"8axobt1l\"},\n",
    "    \n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd1.5\", \"type\": \"position\", \"id\": \"curxv9va\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd1.5\", \"type\": \"counting\", \"id\": \"9j8saxms\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd1.5\", \"type\": \"color_attr\", \"id\": \"ok41tk28\"},\n",
    "    \n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd3\", \"type\": \"counting\", \"id\": \"1tcxc2rz\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd3\", \"type\": \"color_attr\", \"id\": \"znbz2uhj\"},\n",
    "    {\"gen\": \"sd3\", \"eval\": \"sd3\", \"type\": \"position\", \"id\": \"4xuy2o92\"},\n",
    "    \n",
    "    {\"gen\": \"whatsup-a\", \"eval\": \"sd1.5\", \"type\": \"whatsup-a\", \"id\": \"gtqrvf9c\"},\n",
    "    {\"gen\": \"whatsup-a\", \"eval\": \"sd2\", \"type\": \"whatsup-a\", \"id\": \"qtwnqk5c\"},\n",
    "    {\"gen\": \"whatsup-a\", \"eval\": \"sd3\", \"type\": \"whatsup-a\", \"id\": \"2zfmbdgb\"},\n",
    "    \n",
    "    {\"gen\": \"whatsup-b\", \"eval\": \"sd1.5\", \"type\": \"whatsup-b\", \"id\": \"vqudhuh5\"},\n",
    "    {\"gen\": \"whatsup-b\", \"eval\": \"sd2\", \"type\": \"whatsup-b\", \"id\": \"784a6ywm\"},\n",
    "    {\"gen\": \"whatsup-b\", \"eval\": \"sd3\", \"type\": \"whatsup-b\", \"id\": \"72srd0uj\"},\n",
    "    \n",
    "    {\"gen\": \"sd1.5-cnt\", \"eval\": \"sd1.5\", \"type\": \"sd1.5-cnt\", \"id\": \"h315owa1\"},\n",
    "    {\"gen\": \"sd1.5-cnt\", \"eval\": \"sd2\", \"type\": \"sd1.5-cnt\", \"id\": \"pctg0k6y\"},\n",
    "    {\"gen\": \"sd1.5-cnt\", \"eval\": \"sd3\", \"type\": \"sd1.5-cnt\", \"id\": \"jx3fdoii\"},\n",
    "    \n",
    "    {\"gen\": \"sd2-cnt\", \"eval\": \"sd1.5\", \"type\": \"sd2-cnt\", \"id\": \"xc0afsg5\"},\n",
    "    {\"gen\": \"sd2-cnt\", \"eval\": \"sd2\", \"type\": \"sd2-cnt\", \"id\": \"cmlxwe5a\"},\n",
    "    {\"gen\": \"sd2-cnt\", \"eval\": \"sd3\", \"type\": \"sd2-cnt\", \"id\": \"ygjlsq3o\"},\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "def load_all_results():\n",
    "    \"\"\"Preload all results into a dictionary.\"\"\"\n",
    "    results_cache = {}\n",
    "    for run in run_ids:\n",
    "        results_file = os.path.join(RESULTS_DIR, f'{run[\"id\"]}_results.pkl')\n",
    "        if not os.path.exists(results_file):\n",
    "            print(f\"Warning: Results file not found for run {run['id']}\")\n",
    "            continue\n",
    "        \n",
    "        with open(results_file, 'rb') as f:\n",
    "            results_cache[run['id']] = pickle.load(f)\n",
    "    \n",
    "    return results_cache\n",
    "results_cache = load_all_results()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.reset_defaults()\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "# global sizes of font 8\n",
    "plt.rcParams['font.size'] = 6  # Reduced from 8 to make all text smaller\n",
    "plt.rcParams['axes.labelsize'] = 4  # Reduced from 5\n",
    "plt.rcParams['axes.titlesize'] = 6  # Reduced from 8\n",
    "plt.rcParams['legend.fontsize'] = 6  # Reduced from 8\n",
    "plt.rcParams['xtick.labelsize'] = 4  # Reduced from 5\n",
    "plt.rcParams['ytick.labelsize'] = 2  # Reduced further to make y-axis numbers less prominent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_comparison_plots(results_cache):\n",
    "    \"\"\"Create a 1x3 grid of plots comparing timestep weights across task types.\"\"\"\n",
    "    # Set up the plot style\n",
    "    plt.style.use('seaborn-v0_8-paper')  # Use a clean, publication-ready style\n",
    "    w,h=5,1.5\n",
    "    \n",
    "    \n",
    "    # Define colors and linestyles for each model\n",
    "    model_styles = {\n",
    "        'sd1.5': {'color': sns.color_palette(\"colorblind\")[0], 'linestyle': '-'},\n",
    "        'sd2': {'color': sns.color_palette(\"colorblind\")[1], 'linestyle': '-'},\n",
    "        'sd3': {'color': sns.color_palette(\"colorblind\")[2], 'linestyle': '-'}\n",
    "    }\n",
    "    \n",
    "    # Group runs by task type\n",
    "    task_types = ['position', 'counting', 'color_attr', 'whatsup-a', 'whatsup-b', 'sd1.5-cnt', 'sd2-cnt']\n",
    "    models = ['sd1.5', 'sd2', 'sd3']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(task_types), figsize=(w,h), sharey=True)\n",
    "    \n",
    "    # Create plots\n",
    "    for i, task_type in enumerate(task_types):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot each model's weights\n",
    "        for model in models:\n",
    "            # Find runs for this task type and model\n",
    "            relevant_runs = [run for run in run_ids if run['type'] == task_type and run['eval'] == model]\n",
    "            \n",
    "            if not relevant_runs:\n",
    "                continue\n",
    "            \n",
    "            # Plot results for each relevant run\n",
    "            for run in relevant_runs:\n",
    "                if run['id'] not in results_cache:\n",
    "                    continue\n",
    "                \n",
    "                results = results_cache[run['id']]\n",
    "                weights = results['timestep_weights']\n",
    "                # Normalize weights by maximum\n",
    "                weights = weights / weights.max()\n",
    "                timesteps = np.arange(len(weights))\n",
    "                \n",
    "                # Plot weights\n",
    "                ax.plot(timesteps / len(timesteps), weights, \n",
    "                       color=model_styles[model]['color'],\n",
    "                       linestyle=model_styles[model]['linestyle'],\n",
    "                       alpha=0.8,\n",
    "                       label=f\"{model.upper()}\")\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_title(f'{task_type.replace(\"_\", \" \").title()}')\n",
    "        ax.set_xlabel('Timestep')\n",
    "        if i == 0:  # Only leftmost plot\n",
    "            ax.set_ylabel('Weight')\n",
    "        # ax.grid(True, alpha=0.2)\n",
    "        # ax.legend(fontsize=8, frameon=False, loc='upper right')\n",
    "        \n",
    "        if i == 1:\n",
    "            # legend\n",
    "            ax.legend(fontsize=6, frameon=False,)\n",
    "        # Set axis limits\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Add minor grid\n",
    "        # ax.grid(True, which='minor', alpha=0.1)\n",
    "        # ax.grid(True, which='major', alpha=0.2)\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # Format axis ticks to show only 1 decimal digit\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))\n",
    "        ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))\n",
    "    # Create figures directory if it doesn't exist\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "    # set w, h\n",
    "    plt.subplots_adjust(left=0.15, right=0.97, top=0.8, bottom=0.3)\n",
    "    plt.gcf().set_size_inches(w, h)\n",
    "\n",
    "    plt.savefig('figures/timestep_weights_comparison.pdf', pad_inches=0, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def create_accuracy_summary(results_cache):\n",
    "    \"\"\"Create a summary table of accuracies before and after weighting.\"\"\"\n",
    "    print(\"\\nAccuracy Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Task Type':<12} {'Model':<8} {'Original':<10} {'Weighted':<10} {'Improvement':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Store results for averaging\n",
    "    results_by_type = {}\n",
    "    \n",
    "    for task_type in ['position', 'counting', 'color_attr']:\n",
    "        results_by_type[task_type] = []\n",
    "        for model in ['sd1.5', 'sd2', 'sd3']:\n",
    "            runs = [run for run in run_ids if run['type'] == task_type and run['eval'] == model]\n",
    "            for run in runs:\n",
    "                if run['id'] not in results_cache:\n",
    "                    continue\n",
    "                \n",
    "                results = results_cache[run['id']]\n",
    "                orig_acc = results['accuracy_original']\n",
    "                best_acc = results['accuracy_best']\n",
    "                improvement = (best_acc - orig_acc) * 100\n",
    "                \n",
    "                results_by_type[task_type].append({\n",
    "                    'model': model,\n",
    "                    'orig_acc': orig_acc,\n",
    "                    'best_acc': best_acc,\n",
    "                    'improvement': improvement\n",
    "                })\n",
    "                \n",
    "                print(f\"{task_type:<12} {model:<8} {orig_acc:.3f}     {best_acc:.3f}     {improvement:+.1f}%\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"Averages by task type:\")\n",
    "    for task_type, results in results_by_type.items():\n",
    "        if results:\n",
    "            avg_improvement = np.mean([r['improvement'] for r in results])\n",
    "            avg_orig = np.mean([r['orig_acc'] for r in results])\n",
    "            avg_best = np.mean([r['best_acc'] for r in results])\n",
    "            print(f\"{task_type:<12} {'AVG':<8} {avg_orig:.3f}     {avg_best:.3f}     {avg_improvement:+.1f}%\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "create_comparison_plots(results_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def create_comparison_plots(results_cache):\n",
    "    \"\"\"Create plots with timestep weights and accuracy bars.\"\"\"\n",
    "    # Set up the plot style\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    w, h = 6, 1.5\n",
    "    \n",
    "    # Create figure with GridSpec for layout control\n",
    "    fig = plt.figure(figsize=(w, h))\n",
    "    \n",
    "    # Define colors for each model\n",
    "    model_styles = {\n",
    "        'sd1.5': {'color': sns.color_palette(\"colorblind\")[0], 'linestyle': '-'},\n",
    "        'sd2': {'color': sns.color_palette(\"colorblind\")[1], 'linestyle': '-'},\n",
    "        'sd3': {'color': sns.color_palette(\"colorblind\")[2], 'linestyle': '-'}\n",
    "    }\n",
    "    \n",
    "    # task_types = ['position', 'counting', 'color_attr', 'whatsup-a', 'whatsup-b', 'sd1.5-cnt', 'sd2-cnt']\n",
    "    task_types = ['position', 'counting', 'color_attr', 'whatsup-a', 'whatsup-b', 'sd1.5-cnt', 'sd2-cnt']\n",
    "    models = ['sd1.5', 'sd2', 'sd3']\n",
    "    \n",
    "    gs = GridSpec(2, len(task_types), height_ratios=[2, 1], hspace=0.8)\n",
    "    \n",
    "    # Create plots\n",
    "    for i, task_type in enumerate(task_types):\n",
    "        # Top subplot for weights\n",
    "        ax_top = fig.add_subplot(gs[0, i])\n",
    "        # Bottom subplot for accuracies\n",
    "        ax_bottom = fig.add_subplot(gs[1, i])\n",
    "        \n",
    "        accuracies = {'uniform': [], 'learned': []}\n",
    "        \n",
    "        # Plot weights and collect accuracies\n",
    "        for model in models:\n",
    "            relevant_runs = [run for run in run_ids if run['type'] == task_type and run['eval'] == model]\n",
    "            \n",
    "            if not relevant_runs:\n",
    "                continue\n",
    "            \n",
    "            for run in relevant_runs:\n",
    "                if run['id'] not in results_cache:\n",
    "                    continue\n",
    "                \n",
    "                results = results_cache[run['id']]\n",
    "                \n",
    "                # Plot weights\n",
    "                weights = results['timestep_weights']\n",
    "                weights = weights / weights.max()\n",
    "                timesteps = np.arange(len(weights))\n",
    "                \n",
    "                ax_top.plot(timesteps / len(timesteps), weights, \n",
    "                          color=model_styles[model]['color'],\n",
    "                          linestyle=model_styles[model]['linestyle'],\n",
    "                          alpha=0.8,\n",
    "                          label=f\"{model.upper()}\")\n",
    "                \n",
    "                # Store accuracies\n",
    "                accuracies['uniform'].append((model, results['accuracy_original']))\n",
    "                accuracies['learned'].append((model, results['accuracy_best']))\n",
    "        \n",
    "        # Customize weight plot\n",
    "        ax_top.set_title(f'{task_type.replace(\"_\", \" \").title()}')\n",
    "        if i == 0:\n",
    "            ax_top.set_ylabel('Weight')\n",
    "        ax_top.set_xlim(0, 1)\n",
    "        ax_top.set_ylim(0, 1)\n",
    "        if i == 3:\n",
    "            # Set legend below the whole plot\n",
    "            ax_top.legend(fontsize=6, frameon=False, ncol=3, bbox_to_anchor=(0.5, -1.6), loc='center')\n",
    "        # ax_top.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))\n",
    "        # ax_top.yaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))\n",
    "        \n",
    "        if i != 0:\n",
    "            ax_top.set_yticklabels([])\n",
    "        \n",
    "        \n",
    "        # write timestep as x lable\n",
    "        # ax_top.set_xticks(timesteps / len(timesteps))\n",
    "        # ax_top.set_xticklabels(timesteps)\n",
    "        ax_top.set_xlabel('Timestep')\n",
    "        # Plot relative accuracy improvements\n",
    "        x = np.arange(len(models))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Calculate relative improvements\n",
    "        improvements = []\n",
    "        for model in models:\n",
    "            uniform_acc = next((acc for m, acc in accuracies['uniform'] if m == model), np.nan)\n",
    "            learned_acc = next((acc for m, acc in accuracies['learned'] if m == model), np.nan)\n",
    "            if not np.isnan(uniform_acc) and not np.isnan(learned_acc):\n",
    "                rel_improvement = max(0, (learned_acc - uniform_acc) * 100)  # Show 0% if negative\n",
    "            else:\n",
    "                rel_improvement = np.nan\n",
    "            improvements.append(rel_improvement)\n",
    "        \n",
    "        # Plot single bars for improvements with corresponding colors\n",
    "        for idx, (improvement, model) in enumerate(zip(improvements, models)):\n",
    "            if not np.isnan(improvement):\n",
    "                rect = ax_bottom.bar(x[idx], improvement, width, \n",
    "                                   color=model_styles[model]['color'], \n",
    "                                   alpha=1)\n",
    "                # Add value label\n",
    "                if improvement == 0:\n",
    "                    ax_bottom.text(x[idx], improvement,\n",
    "                                 '0%',\n",
    "                                 ha='center', va='bottom', fontsize=3)\n",
    "                else:\n",
    "                    ax_bottom.text(x[idx], improvement,\n",
    "                                 f'+{improvement:.1f}%',\n",
    "                                 ha='center', va='bottom', fontsize=5)\n",
    "        \n",
    "        # Customize accuracy plot\n",
    "        ax_bottom.set_xticks(x)\n",
    "        ax_bottom.set_xticklabels(['1.5', '2', '3'], fontsize=6)\n",
    "        if i == 0:\n",
    "            ax_bottom.set_ylabel('Diff. (%)', position=(-30, 1), labelpad=18)\n",
    "        ax_bottom.yaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))\n",
    "        \n",
    "        # Remove top subplot's bottom ticks\n",
    "        # ax_top.set_xticks([])\n",
    "        \n",
    "        # Get current ticks from bottom plot\n",
    "        ax_bottom.set_yticklabels([])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Remove left spline period\n",
    "        ax_bottom.spines['left'].set_visible(False)\n",
    "        # and left ticks ofc\n",
    "        ax_bottom.set_yticks([])\n",
    "        # remove x little ticks \n",
    "        ax_bottom.tick_params(axis='x', length=0)\n",
    "        \n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(left=0.1, right=0.97, top=0.87, bottom=0.2, wspace=0.3, hspace=10)\n",
    "    plt.gcf().set_size_inches(w, h)\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "    plt.savefig('figures/timestep_weights_comparison.pdf', pad_inches=0, dpi=300)\n",
    "    plt.close()\n",
    "create_comparison_plots(results_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results once\n",
    "\n",
    "# Create visualizations using cached results\n",
    "# create_accuracy_summary(results_cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
